%===============================================================================
\section{Modelos para sistemas não lineares}
\label{sec:nl_models}
%===============================================================================
% ideia aqui é colocar uma pequena introdução sobre modelos.. no mesmo estilo
% que foi para sistemas lineares.

Na seção (\ref{sec:sys_ident_modelling_choosing}) foi apresentado o conceito de modelos para
sistemas lineares. Nesta seção será apresentado de forma análoga o conceito de modelos e tipos de
modelos para sistemas não lineares. Um problema fundamental referente a modelos parametrizados 
refere-se ao fato do parametro poder ou não ser determinado nos valores medidos de entrada e saída
dos dados. \cite{glad_ljung}

Na seção (\ref{sec:nl_models_wiener_hammerstein}) será apresentado um dos modelos mais antigos e que
juntamente com o modelo de Voltera (seção (\ref{sec:nl_models_volterra})) iniciaram o
desenvolvimento dos modelos matemáticos para sistemas não lineares. 

Também será apresentado modelos mais recentes, como as funções radiais de base (seção
(\ref{sec:nl_models_radiais})) derivadas do conceito de redes neurais (\ref{sec:nl_models_neurals}).
Por fim será apresentado o conceito de modelos {\it{NARX}} e duas de suas representações:
Polinomiais (seção (\ref{sec:nl_models_narmax_pol})) e Racionais (seção
(\ref{sec:nl_models_narmax_rat})) que são amplamente utilizadas para caracterização de sistemas não
lineares, devido entre outros fatores aos algoritmos desenvolvidos e a quantidade de sistemas reais
que podem ser descritos por estes modelos.

Um dos passos mais desafiadores na construção de modelos não lineares é a escolha da estrutura de
modelo. Quando este modelo é não linear, existe uma grande quantidade de opções e com isso o perigo
de se escolher um modelo desnecessariamente complexo. Isso se baseia no {\it{principio da
parsemonia}} que basicamente detemina que o modelo deve ser o mais simples possível.
\cite{aguirre_maps}

Uma das primeiras razões para o desenvolvimento de metodos para a escolha de modelos é a grande
dificuldade de trablhar com modelos muito grandes e complexos. Existe o problema destes modelos
serem numericamente mal condicionados. Existe também uma convicção de que estes modelos com muitos
parametros são redundantes e poderiam ser removidos do modelo. Um modelo com um número exessivo de
parametros podem exibir dinamicas que não são observadas no sistema real, desta forma não existe
apenas o problema numérico para estes sistemas, mas também um problema de dinâmica.
\cite{aguirre_jacome}


% ===============================================================================
\subsection{Modelos de Wiener e Hammerstein}
\label{sec:nl_models_wiener_hammerstein}
% Aguirre: 334
% ljung 143
%===============================================================================

Em uma situação onde a dinâmica do sistema pode ser bem descrita por um sistema linear, 
mas existem algumas não linearidades estáticas atreladas a entrada e/ou a saída.
Este será o caso de atuadores serem não lineares como por exemplo: devido a saturação, 
ou se o sensor tem características não lineares. 

Um modelo com não linearidades na entrada é chamado de {\it{modelo de Hammerstein}} e 
para não linearidades na saída chama-se {\it{modelo de Wiener}}. \cite{ljung}

Considere o sistema apresentado em \eqref{eq:nl_linearization_sys} e o caso de 
Hammerstein, tem-se que a função estática não linear $f(\cdot)$ pode ser parametrizado
tanto em termos de parâmetros físicos, como ponto ou nível de saturação, como pode
ser parametrizado por modelo caixa preta.

Na Figura (\ref{fig:nl_models_hammerstein_wiener}) observa-se o diagrama de bloco para os modelos
de Hammerstein e Wiener.


\begin{figure}[htbp]
\center
\scalebox{1} % Change this value to rescale the drawing.
{
	\begin{pspicture}(0,-1.4)(8.949375,1.4)
		\psline[linewidth=0.04cm,arrowsize=0.08cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.0,0.8)(1.2,0.8)
		\psframe[linewidth=0.04,dimen=outer](3.4,1.4)(1.2,0.2)
		\psline[linewidth=0.04cm,arrowsize=0.08cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.4,0.8)(4.6,0.8)
		\psframe[linewidth=0.04,dimen=outer](6.8,1.4)(4.6,0.2)
		\psline[linewidth=0.04cm,arrowsize=0.08cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.8,0.8)(8.0,0.8)
		\psline[linewidth=0.04cm,arrowsize=0.08cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.0,-0.8)(1.2,-0.8)
		\psframe[linewidth=0.04,dimen=outer](3.4,-0.2)(1.2,-1.4)
		\psline[linewidth=0.04cm,arrowsize=0.08cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.4,-0.8)(4.6,-0.8)
		\psframe[linewidth=0.04,dimen=outer](6.8,-0.2)(4.6,-1.4)
		\psline[linewidth=0.04cm,arrowsize=0.08cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.8,-0.8)(8.0,-0.8)
		\usefont{T1}{ptm}{m}{n}
	\rput(0.489375,1.11){u(t)}
	\usefont{T1}{ptm}{m}{n}
	\rput(3.900625,1.11){f(u(t))}
	\usefont{T1}{ptm}{m}{n}
	\rput(7.299375,1.11){y(t)}
	\usefont{T1}{ptm}{m}{n}
	\rput(5.6226563,0.91){Modelo}
	\usefont{T1}{ptm}{m}{n}
	\rput(5.5264063,0.51){Linear}
	\usefont{T1}{ptm}{m}{n}
	\rput(2.2226562,-0.69){Modelo}
	\usefont{T1}{ptm}{m}{n}
	\rput(2.1264062,-1.09){Linear}
	\usefont{T1}{ptm}{m}{n}
	\rput(2.180625,0.71){f()}
	\usefont{T1}{ptm}{m}{n}
	\rput(5.580625,-0.89){f()}
	\usefont{T1}{ptm}{m}{n}
	\rput(7.949375,-0.49){y(t)=f(z(t))}
	\usefont{T1}{ptm}{m}{n}
	\rput(0.489375,-0.49){u(t)}
	\usefont{T1}{ptm}{m}{n}
	\rput(3.876875,-0.49){z(t)}
	\end{pspicture} 
}
\caption{Acima: modelo de Hammerstein. Abaixo: Modelo de Wiener.}
\label{fig:nl_models_hammerstein_wiener}
\end{figure}

%===============================================================================
\subsection{Serie de Volterra}
\label{sec:nl_models_volterra}
% Aguirre 334
%===============================================================================

Um sistema não linear pode ser descrito pela serie de Volterra \eqref{eq:nl_models_volterra}:

\begin{equation}
y(t)=\sum_{j=1}^{\infty}\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}
h_j(\tau_1, ... ,\tau_j) \prod_{i=1}^{j}u(t-\tau_i)d\tau_i
\label{eq:nl_models_volterra}
\end{equation}

Onde $h_j$ são generalizações não lineares da resposta ao impulso $h_1(t)$ . Para
um sistema linear com $j=1$ a equação de Volterra se reduz a integral de convolução.
\cite{aguirre}

Grande dificuldade de utilizar a serie de Volterra \eqref{eq:nl_models_volterra} é que
até para sistemas pouco não lineares, o numero de parâmetros a estimar é grande. Isso
se dá pelo fato da série tentar explicar a saída do sistema apenas baseado nos valores
da entrada deste.

%===============================================================================
\subsection{Redes Neurais}
\label{sec:nl_models_neurals}
% TODO: Search for bib
%===============================================================================
As redes neurais artificiais são compostas por camadas de neuronios interconectados. A saída de um
neurônio com $n$ entradas é apresentado na equação \eqref{eq:nl_models_neural}

\begin{equation}
x=\emph{f}\left ( \sum_{j=1}^{n}\omega_j x_j +b \right )
\label{eq:nl_models_neural}
\end{equation}

Sendo que $b$ (bias) e $\omega_j$ são constantes e $\emph{f}$ é chamada de função de ativação. A
função de ativação mais comum é: \cite{aguirre}

\begin{equation}
\emph{f}(z)=\frac{1}{1+e^{-z}}
\nonumber
\end{equation}


%===============================================================================
\subsubsection{Redes Neurais multicamadas}
\label{sec:nl_models_neurals_multilayer}
%===============================================================================

Uma tipica rede multimamadas pode ser descrita como na Figura
(\ref{fig:nl_models_neural_multilayer}).  Na pratica redes neurais multicamadas tem grande apelo no
reconhecimento de padrões. Do ponto de vista teórico os sistemas de redes multicamadas podem ser
considerados como mapas não lineares onde os elementos das matrizes de peso são os parâmetros.
\cite{narenda_parthasarathy}

\begin{figure}[htbp]
\center
\begin{pspicture}(0,-2.5525)(10.635938,2.5125)
\pscircle[linewidth=0.04,dimen=outer](3.27,2.0025){0.51}
\pscircle[linewidth=0.04,dimen=outer](3.27,0.6025){0.51}
\pscircle[linewidth=0.04,dimen=outer](3.27,-1.3975){0.51}
\usefont{T1}{ptm}{m}{n}
\rput(3.37875,2.0025){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(3.2723436,0.6025){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(3.2723436,-1.3975){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(4.7545314,2.0025){$\gamma$}
\usefont{T1}{ptm}{m}{n}
\rput(4.763125,0.6025){$\gamma$}
\usefont{T1}{ptm}{m}{n}
\rput(4.763125,-1.3975){$\gamma$}
\psdots[dotsize=0.12](0.58,-1.5075)
\psdots[dotsize=0.12](0.58,0.4925)
\psdots[dotsize=0.12](0.58,1.8925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.22,arrowlength=2.0,arrowinset=0.4]{->}(0.58,1.8925)(2.78,1.8925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,0.4925)(2.78,0.4925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,-1.5075)(2.78,-1.5075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,1.8925)(2.78,-1.5075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,1.8925)(2.78,0.4925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,0.4925)(2.78,1.8925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,0.4925)(2.78,-1.5075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,-1.5075)(2.78,1.8925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(0.58,-1.5075)(2.78,0.4925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.22,arrowlength=2.0,arrowinset=0.4]{->}(5.18,1.8925)(7.38,1.8925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,0.4925)(7.38,0.4925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,-1.5075)(7.38,-1.5075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,1.8925)(7.38,-1.5075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,1.8925)(7.38,0.4925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,0.4925)(7.38,1.8925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,0.4925)(7.38,-1.5075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,-1.5075)(7.38,1.8925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=2.0,arrowinset=0.4]{->}(5.18,-1.5075)(7.38,0.4925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.78,2.0925)(4.18,2.0925)
\psframe[linewidth=0.04,dimen=outer](5.18,2.4925)(4.18,1.4925)
\psframe[linewidth=0.04,dimen=outer](5.18,1.0925)(4.18,0.0925)
\psframe[linewidth=0.04,dimen=outer](5.18,-0.9075)(4.18,-1.9075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.78,0.6925)(4.18,0.6925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.78,-1.3075)(4.18,-1.3075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(9.78,2.0725257)(10.18,2.0725257)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(9.78,0.6807505)(10.18,0.6807505)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(9.78,-1.3075)(10.18,-1.3075)
\pscircle[linewidth=0.04,dimen=outer](7.87,2.0025){0.51}
\pscircle[linewidth=0.04,dimen=outer](7.87,0.6025){0.51}
\pscircle[linewidth=0.04,dimen=outer](7.87,-1.3975){0.51}
\usefont{T1}{ptm}{m}{n}
\rput(7.97875,2.0025){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(7.8723435,0.6025){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(7.8723435,-1.3975){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(9.354531,2.0025){$\gamma$}
\usefont{T1}{ptm}{m}{n}
\rput(9.363125,0.6025){$\gamma$}
\usefont{T1}{ptm}{m}{n}
\rput(9.363125,-1.3975){$\gamma$}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.38,2.0925)(8.78,2.0925)
\psframe[linewidth=0.04,dimen=outer](9.78,2.4925)(8.78,1.4925)
\psframe[linewidth=0.04,dimen=outer](9.78,1.0925)(8.78,0.0925)
\psframe[linewidth=0.04,dimen=outer](9.78,-0.9075)(8.78,-1.9075)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.38,0.6925)(8.78,0.6925)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.38,-1.3075)(8.78,-1.3075)
\usefont{T1}{ptm}{m}{n}
\rput(10.382656,2.2025){$y_1$}
\usefont{T1}{ptm}{m}{n}
\rput(10.382656,0.8025){$y_2$}
\usefont{T1}{ptm}{m}{n}
\rput(10.382656,-1.1975){$y_n$}
\usefont{T1}{ptm}{m}{n}
\rput(0.17265625,2.0025){$u_1$}
\usefont{T1}{ptm}{m}{n}
\rput(0.17265625,0.6025){$u_2$}
\usefont{T1}{ptm}{m}{n}
\rput(0.17265625,-1.3975){$u_n$}
\usefont{T1}{ptm}{m}{n}
\rput(0.64390624,-2.3975){Entrada}
\usefont{T1}{ptm}{m}{n}
\rput(4.09375,-2.3975){Camada Oculta}
\usefont{T1}{ptm}{m}{n}
\rput(8.40375,-2.3975){Camada Saída}
\end{pspicture} 
\caption{Rede neural multicamadas.}
\label{fig:nl_models_neural_multilayer}
\end{figure}

A saída do sistema para uma rede multicamada pode ser descrito como em
\eqref{eq:nl_models_neural_multilayer}.

\begin{equation}
y(t)=f_s\left \{ \sum_{i=1}^{m} \omega_i f_i \left ( \sum_{j=1}^{n}\omega_{ij} x_j + b_i \right ) +
b_s \right \}
\label{eq:nl_models_neural_multilayer}
\end{equation}

Sendo que $f_s$ é a função de ativação do neurônio da camada de saída. Esta função não precisa ser
igual a $f_i$, $i=1, \ldots , m$ que por sua vez não precisam ser iguais entre si. $b_s$ é o termo
de polarização do neurônio da camada de saída, $\omega_i$ são os pesos da saída de cada neuronio da
camada oculta e $\omega_{ij}$ são os pesos da entrada $j$, vista pelo $i-$ésimo neurônio da camada
oculta. \cite{aguirre}
%===============================================================================
\subsubsection{Redes Neurais recorrentes}
\label{sec:nl_models_neurals_multilayer}
%===============================================================================
Redes neurais recorrentes, trabalho introduzido por Hopfield em \cite{hopfield} provê uma
alternativa para o reconhecimento de pradrões. O metodo proposto consiste em ter uma rede neural de
apenas uma camada adicionado de uma realimentação com um atraso de tempo como apresetado na Figura
(\ref{fig:nl_models_neural_recurrent}). \cite{narenda_parthasarathy}

\begin{figure}[htbp]
\center
\scalebox{0.85} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-7.3)(11.82,7.3)
\psframe[linewidth=0.04,dimen=outer](9.4,7.3)(8.4,6.3)
\usefont{T1}{ptm}{m}{n}
\rput(9.021093,6.81){$\gamma$}
\psframe[linewidth=0.04,dimen=outer](9.4,5.3)(8.4,4.3)
\usefont{T1}{ptm}{m}{n}
\rput(9.021093,4.81){$\gamma$}
\psframe[linewidth=0.04,dimen=outer](9.4,2.1)(8.4,1.1)
\usefont{T1}{ptm}{m}{n}
\rput(9.021093,1.61){$\gamma$}
\psframe[linewidth=0.04,dimen=outer](6.0,-1.1)(5.0,-2.1)
\usefont{T1}{ptm}{m}{n}
\rput(5.478906,-1.59){$z^{-1}$}
\pscircle[linewidth=0.04,dimen=outer](7.2,6.7){0.4}
\pscircle[linewidth=0.04,dimen=outer](7.2,1.5){0.4}
\pscircle[linewidth=0.04,dimen=outer](7.2,4.7){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(7.2210937,1.41){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(7.2210937,4.61){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(7.2210937,6.61){$\sum$}
\psdots[dotsize=0.12](6.6,6.7)
\psdots[dotsize=0.12](6.6,4.7)
\psdots[dotsize=0.12](6.6,1.5)
\psdots[dotsize=0.12](2.4,6.7)
\psdots[dotsize=0.12](2.4,4.7)
\psdots[dotsize=0.12](2.4,1.5)
\psline[linewidth=0.04cm](2.4,6.7)(6.6,6.7)
\psline[linewidth=0.04cm](2.4,6.7)(6.6,4.7)
\psline[linewidth=0.04cm](2.4,6.7)(6.6,1.5)
\psline[linewidth=0.04cm](2.4,1.5)(6.6,6.7)
\psline[linewidth=0.04cm](2.4,4.7)(6.6,6.7)
\psline[linewidth=0.04cm](2.4,4.7)(6.6,4.7)
\psline[linewidth=0.04cm](2.4,4.7)(6.6,1.5)
\psline[linewidth=0.04cm](2.4,1.5)(6.6,4.7)
\psline[linewidth=0.04cm](2.4,1.5)(6.6,1.5)
\psline[linewidth=0.04cm](9.4,1.5)(10.2,1.5)
\psline[linewidth=0.04cm](10.2,1.5)(10.2,-1.7)
\psline[linewidth=0.04cm](10.2,-1.7)(6.0,-1.7)
\psline[linewidth=0.04cm](5.0,-1.7)(1.2,-1.7)
\psline[linewidth=0.04cm](1.2,-1.7)(1.2,1.5)
\psline[linewidth=0.04cm](1.2,1.5)(2.4,1.5)
\psline[linewidth=0.04cm](7.6,1.5)(8.4,1.5)
\psline[linewidth=0.04cm](6.6,1.5)(6.8,1.5)
\psline[linewidth=0.04cm](7.6,4.7)(8.4,4.7)
\psline[linewidth=0.04cm](6.6,4.7)(6.8,4.7)
\psline[linewidth=0.04cm](6.6,6.7)(6.8,6.7)
\psline[linewidth=0.04cm](7.6,6.7)(8.4,6.7)
\psline[linewidth=0.04cm](9.4,4.7)(11.0,4.7)
\psline[linewidth=0.04cm](11.0,4.7)(11.0,-3.7)
\psline[linewidth=0.04cm](6.0,-3.7)(11.0,-3.7)
\psline[linewidth=0.04cm](9.4,6.7)(11.8,6.7)
\psline[linewidth=0.04cm](11.8,6.7)(11.8,-6.9)
\psline[linewidth=0.04cm](11.8,-6.9)(6.0,-6.9)
\psline[linewidth=0.04cm](5.0,-3.7)(0.6,-3.7)
\psline[linewidth=0.04cm](0.6,-3.7)(0.6,4.7)
\psline[linewidth=0.04cm](0.6,4.7)(2.4,4.7)
\psline[linewidth=0.04cm](2.4,6.7)(0.0,6.7)
\psline[linewidth=0.04cm](0.0,6.7)(0.0,-6.9)
\psline[linewidth=0.04cm](0.0,-6.9)(5.0,-6.9)
\psdots[dotsize=0.12](9.0,3.5)
\psdots[dotsize=0.12](9.0,3.3)
\psdots[dotsize=0.12](9.0,3.1)
\psdots[dotsize=0.12](5.6,-4.9)
\psdots[dotsize=0.12](5.6,-5.1)
\psdots[dotsize=0.12](5.6,-5.3)
\usefont{T1}{ptm}{m}{n}
\rput(10.269531,7.01){$x_1(t+1)$}
\usefont{T1}{ptm}{m}{n}
\rput(10.269531,5.01){$x_2(t+1)$}
\usefont{T1}{ptm}{m}{n}
\rput(10.269531,1.81){$x_n(t+1)$}
\usefont{T1}{ptm}{m}{n}
\rput(1.8095312,7.01){$x_1(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(1.8095312,5.01){$x_2(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(1.8095312,1.81){$x_n(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(4.389531,7.01){{$\omega_i$}}
\psframe[linewidth=0.04,dimen=outer](6.0,-3.1)(5.0,-4.1)
\usefont{T1}{ptm}{m}{n}
\rput(5.478906,-3.59){$z^{-1}$}
\psframe[linewidth=0.04,dimen=outer](6.0,-6.3)(5.0,-7.3)
\usefont{T1}{ptm}{m}{n}
\rput(5.478906,-6.79){$z^{-1}$}
\end{pspicture}  
}
\caption{Rede neural recorrentes.}
\label{fig:nl_models_neural_recurrent}
\end{figure}

%===============================================================================
\subsection{Funções Radiais de Base}
\label{sec:nl_models_radiais}
% Aguirre 337
%===============================================================================

Funções radiais de base  ({\it{RBF - Radial basis functions}}  são uma tradicional tecnica para
interpolação em espaços multidimensional \cite{chen_billings_narmax} e podem ser descritos como
mapeamentos do tipo \eqref{eq:nl_models_rbf}:

\begin{equation}
f(y)-w_0+\sum_{i}w_i \phi (\left \| y-c_i \right \|)
\label{eq:nl_models_rbf}
\end{equation}

Sendo que $y \in \mathbb{R}^{d_e}$ ($d_e$ é conhecido como dimensão de imersão),
$\left \| \cdot \right \|$ é a norma euclidiana, $w_i \in \mathbb(R)$ são pesos, 
$c_i \in \mathbb{R}^{d_e}$ são centros e $\phi(\cdot):\mathbb{R}^+ \to \mathbb{R}$ 
é uma função, normalmente escolhida a priori, como por exemplo: \cite{aguirre}

\begin{equation}
\phi(\left \| y-c_i \right \|)= exp\left ( -\frac{\left \| y-c_i \right \|^2}{\sigma_i^2} \right )
\nonumber
\end{equation}

Outras funções de base usadas são apresentadas na Tabela \ref{table:nl_models_rbf}

\begin{table*}[htbp]
\begin{center}
\caption{Algumas funções Radiais de base comumente usadas.}
\label{table:nl_models_rbf}
\begin{tabular}{ll}
\hline
        Nome & Função   \\
\hline
        Multi quadrática inversa  & $\phi(r)=(r^2+\sigma ^2)^{-1/2}$ \\ 
        Linear                    & $\phi(r)=r$                      \\ 
        Cúbica                    & $\phi(r)=r^3$                    \\ 
        Multiquadrática           & $\phi(r)=\sqrt{r^2+\sigma^2}$    \\ 
        {\it{Thin - plate spline}} & $\phi(r)=r^2\; \text{log}(r)$   \\ 
\hline
\end{tabular}
\end{center}
\end{table*}

Sendo que $r=\left \| y-c_i \right \|$ e $\sigma$ definem a largura do chapéu no caso de
funções gausianas e das multiquadráticas, como pode ser visto na figura (\ref{fig:nl_models_rbf}).

\begin{figure}[htbp]
	\center
	\includegraphics[width=0.8\columnwidth]{figures/nl_models_rbf.eps}
	\caption{Função multiquadráticas inversa para alguns valores de $\sigma$.}
	\label{fig:nl_models_rbf}
\end{figure}

Este tipo de representação tem boas propriedades locais e pode ser interpretada como 
uma técnica de interpolação global. Funções radiais de base são casos particulares
de redes neurais, porem neste caso lineares nos parametros $w_i$.\cite{aguirre} 

No contexto de identificação de sistemas é comum adicionar termos auto-regressivos
lineares, bem como termos de entrada à equação \eqref{eq:nl_models_rbf} resultando em:

\begin{equation}
y(k)=w_0 + \sum_{i}w_i \phi(\left \| \mathbf{y}(k-1)-c_i \right \|)+\sum_{i=1}^{n_y}a_i y(k-i)+\sum_{i=1}^{n_u}b_i u(k-i)+e(k)
\nonumber
\end{equation}

Sendo $\mathbf{y}(k-1)=\begin{bmatrix}
y(k-1) & ... & y(k-n_y) & u(k-1) & ... & u(k-n_u)
\end{bmatrix}$.

%===============================================================================
\section{Modelos NARMAX}
\label{sec:nl_models_narmax}
% Aguirre 343
%===============================================================================

Os modelos {\it{NARX}} (do termo ingles {\it{nonlinear autoregressive model with
exogenous variables}}) são modelos discretos no tempo que caracterizam o valor da 
saída em função dos valor passados da entrada e saída. Algumas vezes, para evitar 
a polarização da estimativa dos parametros adiciona-se termos do ruido no modelo.
Quando isso é feito o modelo passa a ser chamado de um modelo {\it{NARMAX}} (do
termo ingles {\it{nonlinear autoregressive moving average model with exogenous
variables}}) que pode ser representado pla equação \eqref{eq:nl_model_narmax} 
\cite{aguirre}. Este modelo provê uma representação unificada para a descrição de sistemas discretos
não lineares \cite{chen_billings_narmax}

\begin{eqnarray}\nonumber
y(t)&=&F [ y(t-1), ..., y(t-n_y), u(t-1), ... , u(t-n_u),\\
&&  e(t),e(t-1), ... , e(t-n_e) ]
\label{eq:nl_model_narmax}
\end{eqnarray}

Onde $e(t)$ é o ruido e $n_e$ é o maior atraso no modelo do ruido. O modelo apresentado
em \eqref{eq:nl_model_narmax} é bastante genérico, caracterizando uma dificuldade 
para sua utilização. A caracterização da equação $F$ geralmente é feita pelas 
representações polinomial e racional. Um modelo polinomial {\it{NARMAX}} sem atraso
puro de tempo tem a forma apresentada em \eqref{eq:nl_model_narmax_pol}.

\begin{equation}
y(t)=\sum_{i}c_i \prod_{j=1}^{n_y}y(t-j) \prod_{r=1}^{n_u}u(t-r) \prod_{q=0}^{n_e}e(t-q)
\label{eq:nl_model_narmax_pol}
\end{equation}

Os modelos polinomiais {\it{bilineares}} são casos particulares do modelo polinomial 
\eqref{eq:nl_model_narmax_pol} quando todos os termos não lineares são do tipo 
$y(t-i)u(t-j), \; \forall i,j$. \cite{aguirre}

Modelos racionais são formados pela razão entre dois polinomios \eqref{eq:nl_model_narmax_rat}.

\begin{equation}
y(t)=\frac{\sum_{i}c_i \prod_{j=1}^{n_y}y(t-j) \prod_{r=1}^{n_u}u(t-r) \prod_{q=0}^{n_e}e(t-q)}
{\sum_{i}c_i \prod_{j=1}^{n_y}y(t-j) \prod_{r=1}^{n_u}u(t-r) \prod_{q=0}^{n_e}e(t-q)} + e(t)
\label{eq:nl_model_narmax_rat}
\end{equation}

Em função de terem uma estrutura mais floxivel, os modelos racionais opdem vir a ser
mais eficientes na modelagem de certos sistemas quando comparados com modelos polinomiais.
Entretanto, os modelos racionais são mais sensíveis ao ruido. \cite{aguirre}

%===============================================================================
\subsection{Modelo polinomial}
\label{sec:nl_models_narmax_pol}
% Aguirre 343
%===============================================================================
Com relação ao modelo generico polinomial {\it{NARMAX}} apresentado em \eqref{eq:nl_model_narmax_pol}
duas considerações serão levadas em conta:

\begin{enumerate}
\item O sistema tem um atraso puro de tempo $\tau _d$ 
\item Nenhum termo cujo parametro tenha que ser estimado pode depender de $e(t)$.
\end{enumerate}

A segunda consideração implica em tornar $F$ independente de $e(t)$. O que equivale a dizer
que $q=1$ em \eqref{eq:nl_model_narmax_pol} resultando em  \eqref{eq:nl_model_narmax_pol_espec}.

\begin{eqnarray}\nonumber
y(t) &=& F[ y(t-1), ..., y(t-n_y), u(t-\tau_d), ..., u(t-\tau_d-n_u+1),\\
&& e(t-1), ..., e(t-n_e)] +e(t)
\label{eq:nl_model_narmax_pol_espec}
\end{eqnarray}

Sendo que $e(t)$ indica que todos os efeitos não podem ser bem representados por 
$F^l\left [ \cdot  \right ]$, é uma função polinomial de $y(t)$, $u(t)$ e 
$e(t)$ com grau de não linearidade $l\in \mathbb{N}$. Portanto a parte 
não deterministica de \eqref{eq:nl_model_narmax_pol_espec} pode ser espandida como
um somatório de termos com grau de não linearidade variando de $1 \le m \le l$. 
Assim sendo, cada termo de grau $m$ poderá conter um falor de grau $p$ do tipo $y(t-i)$
e um fator de grau $(m-p)$ do tipo $u(t-i)$ sendo multiplicado por um parametro representado 
por $c_{p,m-p}(n_1, ..., n_m$. Resultando em: \cite{aguirre}

\begin{equation}
y(t)=\sum_{m=0}^{l}\sum_{p=0}^{m}\sum_{n1, n_m}^{n_y, n_u}c_{p,m-p}(n_1,...,n_m)\prod_{i=1}^{p}y(t-n_i)\prod_{i=p+1}^{m}u(t-n_i)
\nonumber
\end{equation}


%===============================================================================
\subsection{Modelo Racional}
\label{sec:nl_models_narmax_rat}
% Aguirre 345
%===============================================================================

Um modelo racional {\it{NARMAX}} tem a seguinte forma geral apresentada em 
\eqref{eq:nl_model_narmax_rat} e de forma simplificada como apresentado em
\eqref{eq:nl_model_narmax_rat_simp}.

\begin{equation}
y(k)=\frac{a(\Upsilon)}{b(\Upsilon)}+e(t)
\label{eq:nl_model_narmax_rat_simp}
\end{equation}

Onde:

\begin{equation}
\Upsilon=\left \{ y(t-1), ..., y(t-n_y), u(t-1), ..., u(t-n_u), e(t-1), ..., e(t-n_e)\right \}
\nonumber
\end{equation}

No modelo apresentado em \eqref{eq:nl_model_narmax_rat}, as funções $a(\Upsilon)$ e $b(\Upsilon)$
são polinomios, mas poderiam ser quaisquer funções. Assumindo que o modelo é formado pela razão de
dois polinomios, é conveniente definir o numerador e denominador de \eqref{eq:nl_model_narmax_rat}
como em \eqref{eq:nl_model_narmax_rat_num} e \eqref{eq:nl_model_narmax_rat_den}

\begin{equation}
a(t-1)=\sum_{j=1}^{N_n}p_{nj}\theta_{nj}=\psi _n^T(k-1)\theta_n
\label{eq:nl_model_narmax_rat_num}
\end{equation}


\begin{equation}
b(t-1)=\sum_{j=1}^{N_d}p_{dj}\theta_{dj}=\psi _d^T(k-1)\theta_d
\label{eq:nl_model_narmax_rat_den}
\end{equation}

Sendo $\theta_{nj}$ e $\theta_{dj}$ são os parametros dos regressores, possuindo informações até o
instante $t-1$. Desta forma o valor de parametros a ser estimados é $N_n + N_d$

A equação \eqref{eq:nl_model_narmax_rat_simp} possui não linearidade nos parametros, tornando a
identificação mais complexa por não ser possível utilizar o método dos minimos quadrados para a
estimativa dos parametros. Uma alternativa para este problema é multiplicar a equação
\eqref{eq:nl_model_narmax_rat_simp} pela equação \eqref{eq:nl_model_narmax_rat_den} em ambos os seus
lados. \cite{billings_zhu91}

\begin{equation}
Y(t)=a(t)-y(t)\sum_{j=2}^{den}p_{dj}(t)\theta_{dj}+b(t)e(t)
\nonumber
\end{equation}


\begin{equation}
Y(t)=\sum_{j=1}^{num}p_{nj}(t)\theta_{nj}-\sum_{j=2}^{den}y(t)p_{dj}(t)\theta_{dj}+\xi (t)
\label{eq:nl_model_narmax_rat_linear_param}
\end{equation}

Onde:

\begin{equation}
Y(t)=y(t)p_{d1}\mid_{\theta_{d1}=1} =p_{d1}(t)\frac{a(t)}{b(t)}+p_{d1}(t)e(t)
\nonumber
\end{equation}

e

\begin{equation}
\xi(t)=b(t)e(t)=\left ( \sum_{j=1}^{den}p_{dj}(t)\theta_{dj} \right )e(t)
\nonumber
\end{equation}

Pelo fato de $e(t)$ ser independente de $b(t)$ e ter média nula, tem-se:

\begin{equation}
E\left [ \xi(t) \right ]=E\left [ b(t) \right ]E\left [ e(t) \right ]=0
\label{eq:nl_model_narmax_rat_noise_var}
\end{equation}

A equação \eqref{nl_model_narmax_rat_linear_param} mostra que todos os termos $t(t)p_{dj}(t)$
incluem o termo de ruido $e(t)$ através de $y(t)$ que este por sua vez é altamente relacionado com
$\xi(t)$, o que resultará em polarização dos parametros, mesmo que $e(t)$ seja um ruido branco com
média zero. Este é de certa forma o preço que se paga ao linearizar os parametros e ser possível
utilizar o método dos minimos quadrados. \cite{aguirre}

%===============================================================================
\subsubsection{Estimador usando métodos dos minimos quadrados}
\label{sec:nl_models_narmax_rat_lsm}
% correa 31 - billings e zhu 1991
%===============================================================================


