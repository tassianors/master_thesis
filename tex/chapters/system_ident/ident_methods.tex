%===============================================================================
\section{Métodos de identificação}
\label{sec:sys_ident_methods}
%===============================================================================
% mains idea of this section:
% MQ
% VI
% Modelagem do ruído

Identificação de sistemas é formado por três premissas básicas: o sistema real $\mathcal{S}$ a classe de modelos
escolhida $\mathcal{M}$ e o critério que elenca qual é o melhor modelo dentro da classe, baseado em algum critério.

Métodos de identificação de sistemas nada mais são do que procedimentos, ou algoritmos, para encontrar o mínimo de uma
função custo, chamada critério. Na seção \ref{sec:sys_ident_intro} foi apresentado o critério de minimização mais
utilizado para a identificação:

\begin{equation}
V(\theta)=\frac{1}{N}\sum_{t=1}^{N}\varepsilon ^2(t, \theta)=\frac{1}{N}\varepsilon^T\varepsilon=\frac{1}{N}\left \|
\varepsilon \right \|
\label{eq:si_method_criteria}
\end{equation}

Nesta seção serão apresentados métodos que minimizam a função custo \eqref{eq:si_method_criteria}. A idéia central está
em utilizar o método dos mínimos quadrados. Existem entretanto situações em que o uso dos mínimos quadrados acarreta
erros na estimativa dos parametros, para isso serão apresentados algumas soluções e suas caracteristicas.

%===============================================================================
\subsection{Método dos mínimos quadrados}
\label{sec:si_par_estim_lsm}
%===============================================================================

Existem diversos métodos para a estimativa de parâmetros. O mais conhecido, remete
ao ano de 1809 utilizado por Gauss para determinação da órbita dos planetas. 
\cite{system_identification}. Método este chamado de mínimos quadrados (MMQ).

A regressão linear é tavez o tipo mais simples de modelo paramétrico. A estrutura do modelo
pode ser descrita como abaixo. \cite{system_identification}

\begin{equation}
\hat{y}(t, \theta)=\varphi ^T(t)\theta
\label{eq:si_obj_single_var}
\end{equation}

Onde $\hat{y}(t)$ é chamada de {\it{variável regredida}}.
$\varphi (t)$ é comumente chamado de {\it{variável de regressão}} e $\theta$ é o vetor de
parâmetros a ser identificado.

A partir do erro de predição (\ref{eq:si_estim_prediction_error}) e (\ref{eq:si_obj_single_var}) podemos
redefinir o erro de predição como:

\begin{equation}
\varepsilon (t, \theta)=y(t)-\varphi ^T(t)\theta
\nonumber
\end{equation}

A {\it{estimativa dos mínimos quadrados}} é definido como o vetor $\hat{\theta}$ 
que minimiza a função custo (\ref{eq:si_method_criteria}). O valor de $\hat{\theta}$ que minimiza
esta função custo é dada por: \cite{system_identification}

\begin{equation}
\hat{\theta}=(\varphi ^T \varphi )^{-1}\varphi  ^T y
\label{eq:si_par_etim_lsm_theta}
\end{equation}

Desta forma o mínimo da função custo fica como em:

\begin{equation}
\underset{\theta}{min}\;V(\theta)=V(\hat{\theta})=\frac{1}{N}\left [ y^Ty-y^T\varphi (\varphi ^T \varphi )^{-1}\varphi
^T y \right ]
\end{equation}

O método dos mínimos quadrados é simples de ser aplicado, mas tem o inconveniente de que para que não existam
erros de polarização na estimativa, a variável de regressão $\varphi(t)$ não pode estar correlacionada com o
distúrbio estocástico $\nu(t)$. Assume-se que o sistema real é dado por:

\begin{equation}
y(t)=\varphi^T(t)\theta^*+\nu(t)
\label{eq:si_par_etim_lsm_true_sys}
\end{equation}

Onde $\theta^*$ proporciona:

\begin{equation}
G(q, \theta^*)=G_0(q)
\nonumber
\end{equation}

Para que as estimativas de $\theta$ tendam a $\theta*$ quando o número de amostras tende a infinito é necessário que:

\begin{enumerate}
  \item $E \varphi(t)\varphi^T(t)$ seja não singular.
  \item $E \varphi(t)\nu(t) = 0$
\end{enumerate} 

A primeira condição é normalmente satisfeita. Algumas excessões são: \cite{system_identification}

\begin{itemize}
  \item Se a entrada não for persistentemente excitante.]
  \item Os dados não são afetados por ruído ($\nu(t) \equiv 0$) e a ordem da classe de modelos é escolhida com um valor
  muito alto. O que implica que $A_0(q)$ e $B_0(q)$ possuem fatores comuns.
  \item A entrada $u(t)$ é gerada por uma realimentação de saída linear de ordem baixa.
\end{itemize}

Ao contrario da condição (1), a condição (2) normalmente não é verdadeira. Uma importante excessão é quando $\nu(t)$ é
ruído branco. Para este caso o $\nu(t)$ será descorrelacionado com todos os dados passados e em especial com
$\varphi(t)$.\cite{system_identification}

Quando o número de amostras tende ao infinito e a estimativa tende para um valor diferente de $\theta^*$, este erro é
chamado de {\it{bias}} ou erro de polarização da estimativa.

Para contornar este erro de {\it{bias}} existem algumas opções que serão abordadas em seguida: Utilização de variáveis
instrumentais e modelagem do rúido.

%===============================================================================
\subsection{Método das variáveis instrumentais}
\label{sec:si_par_estim_iv}
%===============================================================================
% bibliografia principal: aguirre e ljung

Como foi visto anteriormente o método dos mínimos quadrados é simples de ser aplicado mas carece
quando existe correlação entre o regressor $\varphi$ e o ruído estocástico $\nu(t)$ causando {\it{bias}} ou erro de
polarização. Nesta seção será apresentado uma breve discussão sobre um dos métodos que se propõe a sanar este
inconveniente do método dos mínimos quadrados: método das vairáveis instrumentais.

Assume-se que $Z(t)$ é uma matriz $n\times n$ que possuem sinais não correlacionados com o 
distúrbio $\nu(t)$. O parâmetro $\theta$ deve obedecer a restrição da equação \eqref{eq:si_par_estim_iv_theta}:

\begin{equation}
\frac{1}{N}\sum_{t=1}^{N}Z(t)\varepsilon (t)=\frac{1}{N}\sum_{t=1}^{N}Z(t)\left [ y(t)-\varphi^T(t)\theta \right ]= 0
\label{eq:si_par_estim_iv_theta}
\end{equation}

Se a dimensão da matriz $Z(t)$ for a mesma dimensão do vetor $\theta$ temos o estimador do método das 
variáveis instrumentais \eqref{eq:si_par_estim_iv}:

\begin{equation}
\hat{\theta}=\left [ \sum_{t=1}^{N}Z(t)\varphi^T(t) \right ]^{-1}\left [  \sum_{t=1}^{N}Z(t)y(t) \right ]
\label{eq:si_par_estim_iv}
\end{equation}

Os elementos da matriz $Z(t)$ são normalmente chamados de instrumentos. O estimador das variáveis instrumentais 
é uma generalização do estimador dos mínimos quadrados, quando $Z(t)=\varphi(t)$. \cite{system_identification}

O estimador de variáveis instrumentais evita a polarização garantido  que o vetor de erro seja não correlacionado
com as variáveis instrumentais. Esta condição e menos restritiva que a condição dos mínimos quadrados para que
não haja erro de polarização. O valor a ser pago por isso envolve: \cite{aguirre}

\begin{enumerate}[(I)]
\item Escolha das variáveis instrumentais.
\item O estimador resultante é assintoticamente não polarizado, ao invés de ser apenas não polarizado. 
\end{enumerate}

Ao escolher as variáveis instrumentais é importante notar que a escolha não deve ser apenas para evitar 
a correlação entre o vetor de erro e os instrumentos. A razão para isso é que as variáveis instrumentais 
devem ser tão correlacionadas quanto possível com os regressores do modelo, caso contrário $Z(t)\varphi(t)^T$ 
seria próxima a singular e sua inversa muito mal condicionada. Portanto os instrumentos devem ser, idealmente, 
pouco correlacionados com o erro e muito correlacionados com os regressores do modelo. \cite{aguirre}

A correção dos instrumentos com os regressores ($E\;Z(t)\varphi(t)^T$) sempre será inferior a correção dos mínimos
quadrados ($E\;\varphi(t)\varphi(t)^T$) onde temos o regressor correlacionado comsigo mesmo . Isso propicia que o uso de
variáveis instrumentais trará uma estimativa com uma varância maior que o método dos minimos quadrados, pois a variância
da estimativa está relacionada com o inverso de $E\;Z(t)\varphi(t)^T$.

%===============================================================================
\subsection{Modelagem do ruído}
\label{sec:si_par_estim_noise_modeling}
%===============================================================================

Como foi apresentado até aqui, tem-se que o método dos mínimos quadrados possui erro de polarização quando existe
correção entre o regressor $\varphi(t)$ e $\nu(t)$, apresentou-se em seguida o método dos minimos quadrados que resolve
este inconveniente ao custo da escolha de uma variável instrumental que não seja correlacionada com o ruído.

Existe entretanto uma segunda forma para lidar com este erro de polarização: Modelagem do ruído. Conhecendo além da
planta $G(q, \theta)$, o modelo do ruído tem-se um conjunto de modelos que melhor representa o sistema real, fazendo com
que o sistema real $\mathcal{S}$ que antes não conseguia ser modelado complentamente apenas por $G(q, \theta)$ passe a
ser representado pelo modelo do sistema completo.

\begin{equation}
\mathcal{M}: \;\;\left \{ y(t)=G(q, \theta)u(t)+ H(q, \theta)e(t) \right \}
\nonumber
\end{equation}

O inconveniente de utilizar a modelagem do rúido é que se o denominador de $G(q, \theta)$ for diferente do dominador de
$H(q, \theta)$ então o sistema não é canonicamente parametrizavel e com isso a função custo não é convexa, adicionando
um grau considerável aos algoritmos para encontrar o mínimo global da equação.


