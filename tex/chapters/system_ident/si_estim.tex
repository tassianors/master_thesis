%===============================================================================
\section{Estimativa de parâmetros}
\label{sec:sys_ident_parameters_estimation}
%===============================================================================

A estimativa dos parâmetros dos modelos dependem de vários fatores. Até agora foi apresentado
a importância dos dados coletados (Seção (\ref{sec:sys_ident_data_acquisition})) e da 
escolha do modelo (Seção (\ref{sec:sys_ident_modelling_choosing})). Nesta seção serão 
apresentadas algumas formas para a estimativa dos parâmetros, bem como algumas de suas
características probabilísticas. 

A identificação é baseada em um conjunto de dados coletados do sistema, um modelo para caracterizá-lo.
Um preditor é uma equação que tenta prever o próximo valor do sistema baseado nos dados passados deste.
Com o preditor atinge-se um conjunto de dados que deve ser muito próximo aos dados verdadeiros
coletados do sistema. Escolhe-se então um método para minimização do erro existente entre
os dados coletados e os dados calculados pelo preditor.

Por fim serão apresentados algumas das características para a estimação quando alguns 
requisitos para a identificação não são atingidos, como por exemplo quando o modelo
escolhido não consegue representar o sistema, ou quando o dados de entrada não são 
suficientemente informativos. Nestas situações teremos erros na estimativa dos parâmetros.
Erros diferentes que serão abordados na seção (\ref{sec:si_par_estim_uncertanties}).


%===============================================================================
\subsection{Preditores}
\label{sec:si_par_estim_preditors}
% ljung pg 68
%===============================================================================

Preditores são equações que predizem qual será o próximo valor de saída do sistema
baseado na família do modelo e nos valores de dados coletados até aquele momento.
Os sinais $y(t)$ e $u(t)$ são os sinais da saída e entrada medidas do sistema real enquanto que o sinal
$\hat{y}(t)$ é o sinal de saída do preditor. A diferênça entre o valor do preditor e o valor do sistema real,
é conhecido como erro de predição:

\begin{equation}
\varepsilon(t)=y(t) - \hat{y}(t, \theta)
\label{eq:si_estim_prediction_error}
\end{equation}


A Figura (\ref{fig:si_estim_pem}) apresenta um diagrama de blocos de como é organizado o método do erro de
predição. Existe um processo que gostaria de ser estimado, para isso escolhe-se uma classe de modelos onde os
parametros a serem ajustados são $\theta$. O preditor é ajustado, baseado nas diferentes possíbilidades de
escolha de $\theta$, este ajuste é feito baseado no erro entre o sistema real ($y(t)$) e a saída do preditor
$\hat{y}(t, \theta)$.


\begin{figure}[htbp]
\center
%\scalebox{1} % Change this value to rescale the drawing.
%{
\begin{pspicture}(0,-2.6)(10.859062,2.6)
\psframe[linewidth=0.04,dimen=outer](4.2,2.6)(1.8,1.4)
\psframe[linewidth=0.04,dimen=outer](5.4,0.2)(3.0,-1.0)
\pscircle[linewidth=0.04,dimen=outer](8.2,1.0){0.4}
\psframe[linewidth=0.04,dimen=outer](8.6,-1.4)(5.6,-2.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(1.8,2.0)(0.0,2.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(4.2,2.0)(10.0,2.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(8.2,1.4)(8.2,2.0)
\psline[linewidth=0.04cm](5.4,-0.4)(8.2,-0.4)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.2,-0.4)(8.2,0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(8.6,-2.0)(9.2,-2.0)
\psline[linewidth=0.04cm](9.2,-2.0)(9.2,1.0)
\psline[linewidth=0.04cm](9.2,1.0)(8.6,1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(4.0,-1.0)(4.0,-2.0)
\psline[linewidth=0.04cm](5.6,-2.0)(4.0,-2.0)
\psline[linewidth=0.04cm](1.2,2.0)(1.2,-0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(3.0,-0.2)(2.0,-0.2)
\psline[linewidth=0.04cm](2.0,-0.2)(2.0,0.8)
\psline[linewidth=0.04cm](2.0,0.8)(5.0,0.8)
\psline[linewidth=0.04cm](5.0,0.8)(5.0,2.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(3.0,-0.6)(1.2,-0.6)
\usefont{T1}{ptm}{m}{n}
\rput(2.9426563,2.11){Processo}
\usefont{T1}{ptm}{m}{n}
\rput(4.2964063,-0.29){Preditor}
\usefont{T1}{ptm}{m}{n}
\rput(7.1589065,-1.65){Algoritmo}
\usefont{T1}{ptm}{m}{n}
\rput(7.177656,-1.95){para minimizar}
\usefont{T1}{ptm}{m}{n}
\rput(7.140625,-2.33){f($\varepsilon(t,\theta)$)}
\usefont{T1}{ptm}{m}{n}
\rput(8.194531,0.99){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(7.9126563,1.51){+}
\usefont{T1}{ptm}{m}{n}
\rput(7.847344,0.51){-}
\usefont{T1}{ptm}{m}{n}
\rput(9.694531,1.31){$\varepsilon(t,\theta)$}
\usefont{T1}{ptm}{m}{n}
\rput(6.514531,2.31){$y(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(0.71453124,2.31){$u(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(7.114531,-0.09){$\hat{y}(t, \theta)$}
\usefont{T1}{ptm}{m}{n}
\rput(4.2632813,-0.69){Ajustável ($\theta$)}
\end{pspicture} 
%}
\caption{Diagrama de blocos para o método do erro de predição}
\label{fig:si_estim_pem}
\end{figure}

Considerando o sistema linear:

\begin{equation}
y(t)=G(q^{-1}, \theta)u(t)+H(q^{-1}, \theta)e(t)
\label{eq:si_par_estim_system}
\end{equation}

Assumindo que $G(0, \theta)=0$, $H(0,\theta)=I$ e que $H^{-1}(q^{-1}, \theta)$ e $H^{-1}(q^{-1},\theta)
G(q^{-1},\theta)$ são assisntóticamente estáveis, se $u(t)$ e $e(p)$ para $p<t$ não forem correlacionados
então o preditor ótimo pode ser apresentado como: \cite{system_identification}

\begin{equation}
\hat{y}(t|t-1, \theta)= H^{-1}(q^{-1}, \theta)G(q^{-1}, \theta)u(t)+\left \{ I-H^{-1}(q^{-1}, \theta) \right
\}y(t)
\label{eq:si_par_estim_predictor}
\end{equation}

\begin{equation}
\varepsilon (t,\theta)=e(t)=H^{-1}(q^{-1}, \theta)\left \{ y(t)-G(q^{-1}, \theta)u(t) \right \}
\nonumber
\end{equation}


%===============================================================================
\subsection{Método dos mínimos quadrados}
\label{sec:si_par_estim_lsm}
%===============================================================================

Existem diversos métodos para a estimativa de parâmetros. O mais conhecido, remete
ao ano de 1809 utilizado por Gauss para determinação da órbita dos planetas. 
\cite{system_identification}. Método este chamado de mínimos quadrados (MMQ).

A partir d erro de predição (\ref{eq:si_estim_prediction_error}) e (\ref{eq:si_obj_single_var}) podemos
redefinir o erro de predição como:

\begin{equation}
\varepsilon (t)=y(t)-\varphi ^T(t)\theta
\nonumber
\end{equation}

A {\it{estimativa dos mínimos quadrados}} de $\theta$ é definido como o vetor $\hat{\theta}$ 
que minimiza a função custo (\ref{eq:si_obj_etim_lsm_v}). O valor de $\hat{\theta}$ que minimiza
esta função custo é dada por: \cite{system_identification}

\begin{equation}
\hat{\theta}=(\varphi ^T \varphi )^{-1}\varphi  ^T y
\label{eq:si_par_etim_lsm_theta}
\end{equation}

Desta forma o mínimo da função custo fica como em:

\begin{equation}
\underset{\theta}{min}\;V(\theta)=V(\hat{\theta})=\frac{1}{2}\left [ y^Ty-y^T\varphi (\varphi ^T \varphi )^{-1}\varphi ^T y \right ]
\end{equation}

O método dos mínimos quadrados é simples de ser aplicado, mas tem o inconveniente de que para que não existam
erros de polarização na estimativa, a variável de regressão $\varphi(t)$ não pode estar correlacionada com o
distúrbio estocástico $\nu(t)$. Assume-se que o sistema real é dado por:

\begin{equation}
y(t)=\varphi^T(t)\theta_0+\nu(t)
\label{eq:si_par_etim_lsm_true_sys}
\end{equation}

A correlação entre os regressores $\varphi(t)$ e $\nu(t)$ somente será nulo se o ruido for branco, o que de
certa forma é bastante restritivo.

\begin{equation}
E(\varphi(t) \; \nu(t)) = 0
\label{eq:si_par_etim_iv_estim}
\end{equation}

satisfeito somente se $\nu(t)$ for ruido branco. Esta desvantagem do método dos mínimos quadrados pode ser
visto como uma oportunidade para a introdução do método de variáveis instrumentais.
\cite{system_identification}

%===============================================================================
\subsection{Método das variáveis instrumentais}
\label{sec:si_par_estim_iv}
%===============================================================================
% bibliografia principal: aguirre e ljung

Como foi visto anteriormente o método dos mínimos quadrados é simples e fácil de ser aplicado mas carece
quando existe correlação entre o regressor $\varphi$ e o ruído estocástico $\nu$. Nesta seção será apresentado
uma breve discussão sobre um dos métodos que se propõem a sanar esta fraqueza do método dos mínimos quadrados:
método das vairáveis instrumentais.

Assume-se que $Z(t)$ é uma matriz $n\times n$ que possuem sinais não correlacionados com o 
distúrbio $\nu(t)$. O parâmetro $\theta$ deve obedecer a restrição da equação \eqref{eq:si_par_estim_iv_theta}:

\begin{equation}
\frac{1}{N}\sum_{t=1}^{N}Z(t)\varepsilon (t)=\frac{1}{N}\sum_{t=1}^{N}Z(t)\left [ y(t)-\varphi^T(t)\theta \right ]= 0
\label{eq:si_par_estim_iv_theta}
\end{equation}

Se a dimensão da matriz $Z(t)$ for a mesma dimensão de $\theta$ temos o estimador do método das 
variáveis instrumentais \eqref{eq:si_par_estim_iv}:

\begin{equation}
\hat{\theta}=\left [ \sum_{t=1}^{N}Z(t)\varphi^T(t) \right ]^{-1}\left [  \sum_{t=1}^{N}Z(t)y(t) \right ]
\label{eq:si_par_estim_iv}
\end{equation}

Os elementos da matriz $Z(t)$ são normalmente chamados de instrumentos. O estimador das variáveis instrumentais 
é uma generalização do estimador dos mínimos quadrados, quando $Z(t)=\varphi(t)$. \cite{system_identification}

O estimador de variáveis instrumentais evita a polarização garantido  que o vetor de erro seja não correlacionado
com as variáveis instrumentais. Esta condição e menos restritiva que a condição dos mínimos quadrados para que
não haja erro de polarização \eqref{eq:si_par_etim_iv_estim}. O valor a ser pago por isso envolve: \cite{aguirre}

\begin{enumerate}[(I)]
\item Escolha das variáveis instrumentais.
\item O estimador resultante é assintoticamente não polarizado, ao invés de ser apenas não polarizado. 
\end{enumerate}

Ao escolher as variáveis instrumentais é importante notar que a escolha não deve ser apenas para evitar 
a correlação entre o vetor de erro e os instrumentos. A razão para isso é que as variáveis instrumentais 
devem ser tão correlacionadas quanto possível com os regressores do modelo, caso contrário $Z(t)\varphi(t)^T$ 
seria próxima a singular e sua inversa muito mal condicionada. Portanto os instrumentos devem ser, idealmente, 
pouco correlacionados com o erro e muito correlacionados com os regressores do modelo. \cite{aguirre}

%===============================================================================
\subsection{Incertezas nos parâmetros estimados}
\label{sec:si_par_estim_uncertanties}
%===============================================================================

O desejo principal para a estimativa de um sistema é que este não possua erros, ou ao menos que este erro, 
se existir, seja o menor possível. Para tanto é necessário caracterizar este tipo de informação do sistema.
Existem dois tipos principais de erro na estimativa de parâmetros. Um deles é o {\it{erro de variância}} e
outro é o {\it{erro de polarização}}.

Seja, $\theta^*$ o limite de convergência para a minimização do erro de predição:

\begin{equation}
\lim_{N \rightarrow \infty }\hat{\theta}_N = \theta^*
\label{eq:si_par_estim_under_theta}
\end{equation}

Sejam os vetores:

\begin{equation}
	Q(q)=\begin{bmatrix}
G_0(q) & H_0(q)
\end{bmatrix}^T
\nonumber
\end{equation}


\begin{equation}
\hat{Q}_N(q)=\begin{bmatrix}
G_0(q, \hat{\theta}_N) & H_0(q, \hat{\theta}_N)
\end{bmatrix}^T
\nonumber
\end{equation}

A qualidade da estimativa pode ser calculada em termos da diferença entre o processo real $Q_0(q)$ e o 
melhor sistema que o modelo pode atingir (quando a quantidade de dados é ilimitada ($N\rightarrow \infty$)) tem-se
então a estimativa do modelo $G_N(q)$.\cite{campestrini, solari}

\begin{equation}
\Delta Q_N(q) \equiv \hat{Q}_N-Q(q)
\label{eq:si_par_estim_under_diff}
\end{equation}

Define-se então:

\begin{equation}
Q^*(q) = \left [ G(q, \theta^*) \; H(q, \theta^*) \right ]^T
\label{eq:si_par_estim_under_q*}
\end{equation}

Entende-se por $Q^*(q)$ como a melhor aproximação que o método pode proporcionar (com $N \rightarrow \infty$) ou 
como sendo a média de todas as estimativas efetuadas.

Adicionando e subtraindo a \eqref{eq:si_par_estim_under_q*} na equação \eqref{eq:si_par_estim_under_diff}
chega-se a definição do erro de polarização e de variância \eqref{eq:si_par_estim_under_errors}.

\begin{equation}
\Delta Q_N(q) \equiv \underset{\text{Erro de variância}}{\underbrace{\hat{Q}_N-Q^*(q)}}+  \underset{\text{Erro de variância}}{\underbrace{Q^*(q)-Q(q)}}
\label{eq:si_par_estim_under_errors}
\end{equation}

A partir da \eqref{eq:si_par_estim_under_errors} compreende-se o erro de polarização como sendo a distância
entre a melhor aproximação possível e o valor real para o parâmetro. Já o erro de variância é a média que cada uma
das estimativas está distante do valor ótimo possível para a estimativa.

\begin{theorem}
Supondo um sistema linear $\mathcal{M}$ parametrizado com em \eqref{eq:si_par_estim_theorem_sys}
e  que os dados de entrada sejam suficientemente informativos.

\begin{equation}
\theta=\begin{bmatrix}
\rho \\ 
\eta 
\end{bmatrix},\;\;
G(q,\theta)=G(q,\rho), \;\;
H(q,\theta)=G(q,\eta)
\label{eq:si_par_estim_theorem_sys}
\end{equation}

Considerando que o sistema opera em malha aberta, ou seja:

\begin{equation}
u(t) \;e\; e_0(t)\text{ são independentes.}
\nonumber
\end{equation}

Seja:

\begin{equation}
\hat{\theta}_N=\begin{bmatrix}
\hat{\rho}_N \\ 
\hat{\eta}_N
\end{bmatrix}
\nonumber
\end{equation}

Obtido pelo método de predição apresentado %%achar a ref 
obtém-se:

\begin{equation}
\begin{matrix}
G(e^{j\omega},\hat{\theta}_N)\rightarrow G_0(e^{j\omega})\\ 
\text{quando:}\\
N\rightarrow \infty
\end{matrix}
\label{eq:si_par_estim_theorem_end}
\end{equation}
\end{theorem}

%===============================================================================
\subsubsection{Covariância nos parâmetros}
\label{sec:si_par_estim_uncertanties_covariance}
%===============================================================================

Um comum meio de medir a qualidade das estimativas é estudar suas propriedades assintóticas. Quando o valor
de $N$ (quantidade de dados) cresce muito, a estimativa pertencerá a alguma distribuição. As propriedades desta
irão determinar a qualidade das estimativas obtidas.\cite{jansson}

\begin{equation}
\begin{matrix}
\sqrt{N}(\hat{\theta}_N-\theta_0) \to \mathcal{N}(0,P) \;\; \text{quando} \;\; N\to \infty \\ \\
\lim_{N\to \infty}N\;E(\hat{\theta}_N-\theta_0)()\hat{\theta}_N-\theta_0)^T=P\\\\
P(\theta_0)=\lambda_0(E\left [ \psi (t,\theta_0)\psi^T(t,\theta_0) \right ])^{-1}\\ \\
\psi (t,\theta_0)=\frac{\partial }{\partial \theta}\hat{y}(t,\theta)\mid_{\theta=\theta_0}
\end{matrix}
\label{eq:si_par_estim_conv_def}
\end{equation}

Não é apenas o tamanho ($N$) do experimento que irá influenciar na qualidade da estimativa.
Na equação \eqref{eq:si_par_estim_cov_spectrum} apresenta-se o espectro onde $\Phi_u$ é o espectro da entrada $u(t)$, 
$\Phi_{ue}$ é o espectro cruzado entre a entrada e o erro $e_0$. Esta distribuição do espectro, $\Phi_{\chi 0}$
também influenciará na qualidade da estimativa, como pode ser visto no Lemma \ref{lemma:si_par_estim_covariance}.

\begin{equation}
\Phi_{\chi 0}=\begin{bmatrix}
\Phi_u & \Phi_{ue}\\ 
\Phi_{ue} & \lambda_0
\end{bmatrix}
\label{eq:si_par_estim_cov_spectrum}
\end{equation}

\begin{lemma}
A inversa da matriz de covariância, $P^{-1}(\theta_0)$, é uma função linear do espectro $\Phi_{\chi 0}$ dado por
\eqref{eq:si_par_estim_cov_lemma}.

\begin{equation}
P^{-1}(\theta_0)=\frac{1}{2\pi \lambda_0}\int_{-\pi}^{\pi}\mathcal{F}(\theta_0)
\Phi_{\chi_0}(\theta_0)\mathcal{F}^*(\theta_0) d \omega
\label{eq:si_par_estim_cov_lemma}
\end{equation}

Onde $\mathcal{F}(q, \theta_0)=\left [ \mathcal{F}_u(q, \theta_0) \;\;\;\mathcal{F}_e(q, \theta_0) \right ]$ e:

\begin{equation}
\mathcal{F}_u(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} G(\theta_0)}{\mathrm{d} \theta} 
\nonumber
\end{equation}

\begin{equation}
\mathcal{F}_e(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} H(\theta_0)}{\mathrm{d} \theta}
\nonumber
\end{equation}
\label{lemma:si_par_estim_covariance}
\end{lemma}

Como $P$ é a medida do tamanho do erro nos parâmetros, o Lemma \ref{lemma:si_par_estim_covariance} mostra
como este erro é relacionado com o espectro $\Phi_{\chi 0}$. O espectro cruzado $\Phi_{ue}$ é zero quando 
o sistema esta operando em laço aberto. \cite{jansson}

%===============================================================================
\subsubsection{Margens de confiança das estimativas}
\label{sec:si_par_estim_uncertanties_confidence_bounds}
%===============================================================================

A partir da distribuição assintótica normal em \eqref{eq:si_par_estim_conv_def} segue:

\begin{equation}
(\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0)\rightarrow \chi^2(n)\;\; \text{quando}\; N\to \infty
\nonumber
\end{equation}

Com $P_N=P/N$ e onde \eqref{eq:si_par_estim_confidence_region} é a região de confiança, onde assintoticamente
inclui $\theta_0$ com probabilidade $\alpha$. Desta forma tem-se que as estimativas estarão centradas em $\theta_0$
e com probabilidade $\alpha$ estarão contidas em uma esfera definida por $P_N$ e $\chi_{\alpha}^2(N)$

\begin{equation}
U_\theta=\left \{ \theta \mid (\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0) \le \chi^2_{\alpha}(n)  \right \}
\label{eq:si_par_estim_confidence_region}
\end{equation}

Na Figura (\ref{fig:si_covar_elipse}) é apresentado um exemplo de região de confiança para um $\chi$ de 95\%. O ponto em destaque
é a média de todas as estimativas e está localizado no centro da região de confiança.

\begin{figure}[htbp]
	\center
	\includegraphics[width=0.8\columnwidth]{figures/si_covar_elipse.eps}
	\caption{Estimativas de um sistema e a região de confiança para $\chi$ de 95\%}
	\label{fig:si_covar_elipse}
\end{figure}



