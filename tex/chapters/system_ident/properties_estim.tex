%===============================================================================
\section{Propriedades estatisticas da estimativa}
\label{sec:sys_ident_prop_estim}
%===============================================================================
% mains idea of this section:
%TODO: reescrever isso aqui
 
%Durante a identificação de um sistema, a ordem de excitação do sinal aplicado e a quantidade de dados que colata-se do
%sistema, podem proporcionar resultados diferentes para os esperimentos realizados. É importante entender o que e onde
%estes parâmetros afetam na resposta final da identificação, para que consiga-se evita-los ou ao menos entende-los e
%saber o quão confiável foi a estimativa obtida.
%
%As propriedades estatisticas das estimativas trazem consigo algumas informações extremamente úteis para a identificação
%de sistemas. Nesta seção serão abordadas algumas destas caracteristicas e o que elas dizem, ou permitem dizer sobre a
%identificação do sistema.


%===============================================================================
\subsection{Conjunto de dados}
\label{sec:si_par_estim_data_z}
%===============================================================================

O conjunto de dados coletados do sistema pode ser descrito como em 

\begin{equation}
Z^N=\left \{ u(1),y(1),...,n(N),y(N) \right \}
\label{eq:si_par_estim_data_set}
\end{equation}
é o ponto de partida básico para a identificação de sistemas. Análise equivale a assumir certas propriedades
sobre os dados calculando as proprerties resultantes de $\hat{\theta}_N$. Desde que a análise de $\hat{\theta}_N$ será
levado para $N \to \infty$, é natural que as condições para os dados, sejam relativos a um conjunto infinido
$Z^{\infty}$. \cite{ljung}

Assume-se que o conjunto de dados $Z^{\infty}$ é tal que para alguns filtros da forma $\left \{ d_t^{(i)}(k) \right \}$:

\begin{align}\nonumber
y(t)&=\sum_{k=1}^{\infty}d_t^{1}(k)r(t-k)+\sum_{k=0}^{\infty}d_t^{2}(k)e_0(t-k)\\
\label{eq:si_properties_data_D1}\\
\nonumber
u(t)&=\sum_{k=0}^{\infty}d_t^{3}(k)r(t-k)+\sum_{k=0}^{\infty}d_t^{4}(k)e_0(t-k)
\end{align}
onde:

\begin{enumerate}
  \item $\left \{ r(t) \right \}$ é uma entrada externa, deterministica e limitada em amplitude.
  \item $\left \{ e_0(t) \right \}$ é uma sequencia randômica independente com média zero, limitada em amplitude e
  momentos com ordem $4+\delta$ para algum $\delta>0$.

Alem disso:
  \item a família de filtros $\left \{ d_t^{(i)}(k) \right \}_{k=1}^{\infty}$, $i=1 - 4$; $t=1,2,\ldots $ é
  uniformemente estável.
  \item Os sinais  $\left \{ y(t) \right \}$ e  $\left \{ u(t) \right \}$ são juntamente quasi-estacionarios.  
\end{enumerate}

%===============================================================================
\subsection{Consistência e identificabilidade}
\label{sec:si_par_estim_consistency}
%===============================================================================

Suponha que a o sistema real possa ser descrito por $\mathcal{S}$ (equação \eqref{eq:si_intro_true_system}) e o
modelo $\mathcal{M}$ seja como apresentado em \eqref{eq:si_intro_model} e além disso que $\mathcal{S} \in \mathcal{M}$.

Os resultados básicos de consistência são quase imediatos: \cite{ljung}

\begin{theorem}
Suponha que o conjunto de dados $Z^{\infty}$ está sugeito ao que foi apresentado na seção
(\ref{sec:si_par_estim_data_z}) e que $\mathcal{S} \in \mathcal{M}$. Assume-se que $Z^{\infty}$ é suficientemente
informativo com respeito a $\mathcal{M}$. Se o sinal de entrada contém a saída por realimentação então também se assume
que exita um delay tanto em $G_0(z)$ e $G(z,\theta)$. Então:

\begin{equation}
D_c=D_T(\mathcal{S,M})
\label{eq:si_par_estim_under_theta}
\end{equation}
onde 

\begin{equation}
D_c=\underset{\theta \in D_{\mathcal{M}}}{arg\;min}V(\theta)=\left \{ \theta \;|\; \theta \in D_{\mathcal{M}},
\;\;V(\theta)=\underset{\theta' \in D_{\mathcal{M}}}{min} V(\theta') \right \}
\label{eq:si_par_estim_dc}
\end{equation}
e

\begin{align*}
D_T(\mathcal{S,M})=\{ \theta \in D_{\mathcal{M}} \; |\;\; & G(e^{j\omega}, \theta)=G_0(e^{j\omega});\\
 & H(e^{j\omega}, \theta)=H_0(e^{j\omega}); -\pi \le \omega \le \pi \}
\end{align*}

Se em adição, a classe de modelos é globalmente identificavel em $\theta_0 \in D_{\mathcal{M}}$, então:

\begin{equation}
D_c=\{\theta_0\}
\end{equation}

Desta forma e como 

\begin{equation}
\hat{\theta}_N\to D_c, \text{quando } N\to \infty
\end{equation}
tem-se que as funções de transferência obedecem a:

\begin{align}\nonumber
G(e^{j\omega}, \hat{\theta}_N) &\to G_0(e^{j\omega}), \\
\\ \nonumber 
H(e^{j\omega}, \hat{\theta}_N) &\to H_0(e^{j\omega}), \; \text{quando } N \to \infty
\end{align}

\label{theorem:si_ljung8.3}
\end{theorem}


%===============================================================================
\subsection{Incertezas mas estimativas dos parâmetros}
\label{sec:si_par_estim_uncertanties}
%===============================================================================

O desejo principal para a identificação de sistemas é que este não possua erros, ou ao menos que este
erro, se existir, seja o menor possível, ou limitado dentro de uma região conhecida. Para tanto é necessário
caracterizar este tipo de informação do sistema. Existem dois tipos principais de erros na estimativa de parâmetros. Um
deles é o {\it{erro de variância}} e outro é o {\it{erro de polarização}}.

Seja, $\theta_0$ o limite de convergência para a minimização do erro de predição:

\begin{equation}
\lim_{N \rightarrow \infty }\hat{\theta}_N = \theta_0
\label{eq:si_par_estim_under_theta}
\end{equation}

Sejam os vetores:

\begin{equation}
Q(q)=\begin{bmatrix}
G_0(q) & H_0(q)
\end{bmatrix}^T
\nonumber
\end{equation}


\begin{equation}
\hat{Q}_N(q)=\begin{bmatrix}
G(q, \hat{\theta}_N) & H(q, \hat{\theta}_N)
\end{bmatrix}^T
\nonumber
\end{equation}

A qualidade da estimativa pode ser calculada em termos da diferença entre o processo real $Q_0(q)$ e o 
melhor sistema que o modelo pode atingir (quando a quantidade de dados é ilimitada ($N\rightarrow \infty$)) tem-se
então a estimativa do modelo $G_N(q)$.\cite{campestrini, solari}

\begin{equation}
\Delta Q_N(q) \equiv \hat{Q}_N(q)-Q(q)
\label{eq:si_par_estim_under_diff}
\end{equation}

Define-se então:

\begin{equation}
Q^*(q) = \left [ G(q, \theta^*) \; H(q, \theta^*) \right ]^T
\label{eq:si_par_estim_under_q*}
\end{equation}

Entende-se por $Q^*(q)$ como a melhor aproximação que o método pode proporcionar quando a quantidade de dados coletados
$N \rightarrow \infty$.

Adicionando e subtraindo a \eqref{eq:si_par_estim_under_q*} na equação \eqref{eq:si_par_estim_under_diff}
chega-se à definição dos erros de polarização e de variância:

\begin{equation}
\Delta Q_N(q) \equiv \underset{\text{Erro de variância}}{\underbrace{\hat{Q}_N(q)-Q^*(q)}}+  \underset{\text{Erro de
polarização}}{\underbrace{Q^*(q)-Q(q)}}
\label{eq:si_par_estim_under_errors}
\end{equation}

Onde $Q^*(q)=\left [ G(q, \theta^*) \;\;\; H(q, \theta^*)\right ]$.

A partir da equação  \eqref{eq:si_par_estim_under_errors} compreende-se o erro de polarização como sendo a distância
entre a melhor aproximação possível e o valor real para o parâmetro. Já o erro de variância é a média que cada uma
das estimativas está distante do valor ótimo possível para a estimativa, que é tão menor quanto mais dados forem usados
para a identificação.

Se o experiemnto é realizado em malha aberta, ou sej, quando $u(t)$ e $e(t)$ são descorrelacionados e se $G(z, \theta)$
e $H(z, \theta)$ são parametrizados de forma indenpente, ou seja,

\begin{equation}
\theta=\begin{bmatrix}
\rho\\ 
\eta 
\end{bmatrix},\;\; G(z, \theta) = G(z,\rho), \;\; H(z, \theta)=H(z, \eta)
\nonumber
\end{equation}

então $G_0(z)$ pode ser estimada de forma consistente, independente de $H_0(z)$ pertencer ou não à classe de modelos
\cite{ljung}. Neste caso, o erro entre o processo real e o modelo estimado é dado pelos erros de polarização e de
variância da estimativa \cite{gevers2006, campestrini}, onde

\begin{align*}
\text{Erro de variância} &= G(z, \hat{\theta}_N)-G(z, \theta^*)\\ 
\text{Erro de polarização}  &= G(z, \theta^*)-G_0(z) 
\end{align*}

%===============================================================================
\subsubsection{Covariância nos parâmetros}
\label{sec:si_par_estim_uncertanties_covariance}
%===============================================================================

Um meio comum de medir a qualidade das estimativas é estudar suas propriedades assintóticas. Quando o valor
de $N$ (quantidade de dados) cresce muito, a estimativa pertencerá a alguma distribuição. As propriedades desta
irão determinar a qualidade das estimativas obtidas.\cite{jansson}

\begin{equation}
\begin{matrix}
\sqrt{N}(\hat{\theta}_N-\theta_0) \to \mathcal{N}(0,P) \;\; \text{quando} \;\; N\to \infty ,\\

\lim_{N\to \infty}N\;E(\hat{\theta}_N-\theta_0)(\hat{\theta}_N-\theta_0)^T \overset{\underset{\mathrm{\Delta}}{\,}}{=} P(\theta_0),\\

\text{onde } P(\theta_0)=\lambda_0(E\left [ \psi (t,\theta_0)\psi^T(t,\theta_0) \right ])^{-1},\\

\psi (t,\theta_0)=\frac{\partial }{\partial \theta}\hat{y}(t,\theta)\mid_{\theta=\theta_0}
\end{matrix}
\label{eq:si_par_estim_conv_def}
\end{equation}

Neste caso, $\mathcal{N}(0, P_\theta)$ denota uma distribuição normal com média 0 e variância $P_\theta$. Asssim quando
o sistema real pertence à classe  de modelos escolhida a estimatica converge para $\theta_0$ e a covariância da
estimativa é dada por:

\begin{equation}
\text{Cov}\hat{\theta}_N \sim \frac{1}{N}P_\theta
\end{equation}

Não é apenas o tamanho ($N$) do experimento que irá influenciar na qualidade da estimativa. Introduz-se o espectro:

\begin{equation}
\Phi_{\chi 0}=\begin{bmatrix}
\Phi_u & \Phi_{ue}\\ 
\Phi_{ue} & \lambda_0
\end{bmatrix}
\label{eq:si_par_estim_cov_spectrum}
\end{equation}


onde $\Phi_u$ é o espectro da entrada $u(t)$, $\Phi_{ue}$ é o espectro cruzado entre a entrada $u(t)$ e o erro $e(t)$.
A distribuição da frequência do espectro $\Phi_{\chi 0}$ também influenciará na qualidade da
estimativa, como pode ser visto no Lemma \ref{lemma:si_par_estim_covariance}.

\begin{lemma}
A inversa da matriz de covariância, $P^{-1}(\theta_0)$, é uma função linear do espectro $\Phi_{\chi 0}$ dado por
\eqref{eq:si_par_estim_cov_lemma}.

\begin{equation}
P^{-1}(\theta_0)=\frac{1}{2\pi \lambda_0}\int_{-\pi}^{\pi}\mathcal{F}(\theta_0)
\Phi_{\chi_0}(\theta_0)\mathcal{F}^*(\theta_0) d \omega
\label{eq:si_par_estim_cov_lemma}
\end{equation}

Onde $\mathcal{F}(q, \theta_0)=\left [ \mathcal{F}_u(q, \theta_0) \;\;\;\mathcal{F}_e(q, \theta_0) \right ]$ e:

\begin{equation}
\mathcal{F}_u(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} G(\theta_0)}{\mathrm{d} \theta} 
\nonumber
\end{equation}

\begin{equation}
\mathcal{F}_e(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} H(\theta_0)}{\mathrm{d} \theta}
\nonumber
\end{equation}
\label{lemma:si_par_estim_covariance}
\end{lemma}

Como $P$ é a medida do tamanho do erro nos parâmetros, o Lemma \ref{lemma:si_par_estim_covariance} mostra
como este erro é relacionado com o espectro $\Phi_{\chi 0}$. O espectro cruzado $\Phi_{ue}$ é zero quando 
o sistema esta operando em laço aberto. \cite{jansson}

%===============================================================================
\subsubsection{Margens de confiança das estimativas}
\label{sec:si_par_estim_uncertanties_confidence_bounds}
%===============================================================================

A partir da distribuição assintótica normal em \eqref{eq:si_par_estim_conv_def} segue:

\begin{equation}
(\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0)\rightarrow \chi^2(n)\;\; \text{quando}\; N\to \infty
\nonumber
\end{equation}

Com $P_N=P/N$ e onde \eqref{eq:si_par_estim_confidence_region} é a região de confiança, onde assintoticamente
inclui $\theta_0$ com probabilidade $\alpha$. Desta forma tem-se que as estimativas estarão centradas em
$\hat{\theta}_N$ e com probabilidade $\alpha$ estarão contidas em uma esfera definida por $P_N$ e $\chi_{\alpha}^2(N)$
\cite{jansson}:

\begin{equation}
U_\theta=\left \{ \theta \mid (\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0) \le \chi^2_{\alpha}(n)
\right \}
\label{eq:si_par_estim_confidence_region}
\end{equation}

Na Figura (\ref{fig:si_covar_elipse}) é apresentado um exemplo de região de confiança para um $\chi$ de 95\%. O ponto
em destaque é a média de todas as estimativas e está localizado no centro da região de confiança.

\begin{figure}[htbp]
	\center
	\includegraphics[width=0.8\columnwidth]{figures/si_covar_elipse.eps}
	\caption{Estimativas de um sistema e a região de confiança para $\chi$ de 95\%}
	\label{fig:si_covar_elipse}
\end{figure}


