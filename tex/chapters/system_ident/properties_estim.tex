%===============================================================================
\section{Propriedades estatísticas da estimativa}
\label{sec:sys_ident_prop_estim}
%===============================================================================
 
Nesta seção tem-se o objetivo de descrever algumas das propriedades estatísticas relevantes para a identificação de
sistemas. Tem-se o intuito de formalizar quais são os erros que podem estar contidos nas estimativas obtidas e quais são
as condições para evitá-las, ou reduzi-las.

Conhecer a qualidade das estimativas que são obtidas de um processo de identificação e suas propriedades estatísticas
são de suma importância para aplicações de engenharia. O conhecimento do valor de $\theta$ sem a informação de
quão confiável esta informação é pouco útil em muitas aplicações. 

%===============================================================================
\subsection{Conjunto de dados}
\label{sec:si_par_estim_data_z}
%===============================================================================

O conjunto de dados coletados do sistema pode ser descrito como em:

\begin{equation}
Z^N=\left \{ u(1), y(1), ... , u(N),y(N) \right \}
\label{eq:si_par_estim_data_set}
\end{equation}
e é o ponto de partida básico para a identificação de sistemas. As propriedades calculadas de $\hat{\theta}_N$ para um
certo montante de dados serão levadas junto para quando $N \to \infty$, é natural que as condições sobre os dados,
sejam relativos a um conjunto infinito $Z^{\infty}$ \cite{ljung}.

Assume-se que o conjunto de dados $Z^{\infty}$ é tal que para alguns filtros da forma $\left \{ d_t^{(i)}(k) \right \}$:

\begin{align}\nonumber
y(t)&=\sum_{k=1}^{\infty}d_t^{1}(k)r(t-k)+\sum_{k=0}^{\infty}d_t^{2}(k)e_0(t-k)\\
\label{eq:si_properties_data_D1}\\
\nonumber
u(t)&=\sum_{k=0}^{\infty}d_t^{3}(k)r(t-k)+\sum_{k=0}^{\infty}d_t^{4}(k)e_0(t-k)
\end{align}
onde:

\begin{enumerate}
  \item $\left \{ r(t) \right \}$ é uma entrada externa, determinística e limitada em amplitude.
  \item $\left \{ e_0(t) \right \}$ é uma sequência randômica independente com média zero, limitada em amplitude e
  momentos com ordem $4+\delta$ para algum $\delta>0$.

Além disso:
  \item a família de filtros $\left \{ d_t^{(i)}(k) \right \}_{k=1}^{\infty}$, $i=1 - 4$; $t=1,2,\ldots $ é
  uniformemente estável.
  \item os sinais  $\left \{ y(t) \right \}$ e  $\left \{ u(t) \right \}$ são juntamente quasi-estacionários.  
\end{enumerate}

%===============================================================================
\subsection{Consistência e identificabilidade}
\label{sec:si_par_estim_consistency}
%===============================================================================

Suponha que a o sistema real possa ser descrito por $\mathcal{S}$ (equação \eqref{eq:si_intro_true_system}) e o
modelo $\mathcal{M}$ seja como apresentado em \eqref{eq:si_intro_model} e além disso que $\mathcal{S} \in \mathcal{M}$.
Os resultados básicos de consistência são quase imediatos e consistem em determinar que quando o sistema real consegue
ser representado pela classe de modelos, então não haverá erro de polarização nas estimativas obtidas. O teorema a
seguir apresenta estas conclusões \cite{ljung}:

\begin{theorem}
Suponha que o conjunto de dados $Z^{\infty}$ está sujeito ao que foi apresentado na Seção
\ref{sec:si_par_estim_data_z} e que $\mathcal{S} \in \mathcal{M}$. Assume-se que $Z^{\infty}$ é suficientemente
informativo com respeito a $\mathcal{M}$. Se o sinal de entrada contém a saída por realimentação então também se assume
que exista um atraso de tempo tanto em $G_0(q)$ quanto em $G(q,\theta)$. Então:

\begin{equation}
D_c=D_T(\mathcal{S,M})
\label{eq:si_par_estim_under_theta}
\end{equation}
onde 

\begin{equation}
D_c=\underset{\theta \in D_{\mathcal{M}}}{arg\;min}V(\theta)=\left \{ \theta \;|\; \theta \in D_{\mathcal{M}},
\;\;V(\theta)=\underset{\theta' \in D_{\mathcal{M}}}{min} V(\theta') \right \}
\label{eq:si_par_estim_dc}
\end{equation}
e

\begin{align*}
D_T(\mathcal{S,M})=\{ \theta \in D_{\mathcal{M}} \; |\;\; & G(e^{j\omega}, \theta)=G_0(e^{j\omega});\\
 & H(e^{j\omega}, \theta)=H_0(e^{j\omega}); -\pi \le \omega \le \pi \}
\end{align*}

Se além disso, a classe de modelos é globalmente identificável em $\theta_0 \in D_{\mathcal{M}}$, então:

\begin{equation}
D_c=\{\theta_0\}
\end{equation}

Desta forma, e como:

\begin{equation}
\hat{\theta}_N\to D_c, \text{quando } N\to \infty
\end{equation}
tem-se que as funções de transferência obedecem a:

\begin{align}\nonumber
G(e^{j\omega}, \hat{\theta}_N) &\to G_0(e^{j\omega}), \\
\\ \nonumber 
H(e^{j\omega}, \hat{\theta}_N) &\to H_0(e^{j\omega}), \; \text{quando } N \to \infty
\end{align}

\label{theorem:si_ljung8.3}
\end{theorem}

%===============================================================================
\subsection{Incertezas nas estimativas dos parâmetros}
\label{sec:si_par_estim_uncertanties}
%===============================================================================

Um dos principais desejos para a identificação de sistemas é que este não possua erros, ou ao menos que este
erro, se existir, seja o menor possível e limitado dentro de uma região conhecida. Para tanto é necessário
introduzir os conceitos de {\it{erro de variância}} e {\it{erro de polarização}}.

Seja $\theta^*$ o limite de convergência para a minimização do erro de predição:

\begin{equation}
\lim_{N \rightarrow \infty }\hat{\theta}_N = \theta^*
\label{eq:si_par_estim_under_theta}
\end{equation}
e sejam os vetores:

\begin{equation}
Q_0(q)=\begin{bmatrix}
G_0(q) & H_0(q)
\end{bmatrix}^T
\nonumber
\end{equation}

\begin{equation}
\hat{Q}_N(q)=\begin{bmatrix}
G(q, \hat{\theta}_N) & H(q, \hat{\theta}_N)
\end{bmatrix}^T
\nonumber
\end{equation}

A qualidade da estimativa pode ser calculada em termos da diferença entre o processo real $Q_0(q)$ e o melhor sistema
que o modelo pode atingir (quando a quantidade de dados é ilimitada ($N\rightarrow \infty$)) \cite{campestrini, solari}.

\begin{equation}
\Delta Q_N(q) \equiv \hat{Q}_N(q)-Q_0(q)
\label{eq:si_par_estim_under_diff}
\end{equation}

Define-se então:

\begin{equation}
Q^*(q) = \left [ G(q, \theta^*) \; H(q, \theta^*) \right ]^T
\label{eq:si_par_estim_under_q*}
\end{equation}

Entende-se por $Q^*(q)$ como a melhor aproximação que o método pode proporcionar quando a quantidade de dados coletados
$N \rightarrow \infty$.  Adicionando e subtraindo a equação \eqref{eq:si_par_estim_under_q*} na equação
\eqref{eq:si_par_estim_under_diff} chega-se à definição dos erros de polarização e de variância:

\begin{equation}
\Delta Q_N(q) \equiv \underset{\text{Erro de variância}}{\underbrace{\hat{Q}_N(q)-Q^*(q)}}+  \underset{\text{Erro de
polarização}}{\underbrace{Q^*(q)-Q_0(q)}}
\label{eq:si_par_estim_under_errors}
\end{equation}

A partir da equação  \eqref{eq:si_par_estim_under_errors} compreende-se o erro de polarização como sendo a distância
entre a melhor aproximação possível e o valor real para o parâmetro. Já o erro de variância é a média que cada uma
das estimativas está distante do valor ótimo possível para a estimativa, que é tão menor quanto mais dados forem usados
para a identificação.

Seguidamente é mais importante ter uma boa estimativa da função de transferência da planta $G(\cdot)$ do que do filtro
do ruído $H(\cdot)$ \cite{ljung}. Suponha que o conjunto de modelos: 

\begin{equation}
\mathcal{G}=\left \{ G(e^{j\omega},\theta) \;|\; \theta \in D_{\mathcal{M}} \right \}
\nonumber
\end{equation}
é abrangente o suficiente para que 

\begin{equation}
G_0(q) \in \mathcal{G}
\end{equation}
mas a função de transferência do filtro real do ruído $H_0(q)$ não consegue ser totalmente descrita pela classe de
modelos. Desta forma $\mathcal{S} \notin \mathcal{M}$ e supondo que o conjunto de dados $Z^{\infty}$ esteja sujeito ao
que foi descrito na Seção \ref{sec:si_par_estim_data_z} e que este seja suficientemente informativo.
Se o experimento é realizado em malha aberta, ou seja, quando $u(t)$ e $e(t)$ são descorrelacionados
e se $G(q, \theta)$ e $H(q, \theta)$ são parametrizados de forma independente:

\begin{equation}
\theta=\begin{bmatrix}
\rho\\ 
\eta 
\end{bmatrix},\;\; G(q, \theta) = G(q,\rho), \;\; H(q, \theta)=H(q, \eta)
\label{eq:si_par_estim_independent_param}
\end{equation}
então $G_0(q)$ pode ser estimada de forma consistente, independente de $H_0(q)$ pertencer ou não à classe de modelos
\cite{ljung}. Neste caso, o erro entre o processo real e o modelo estimado é dado pelos erros de polarização e de
variância da estimativa \cite{gevers2006, campestrini}, onde:

\begin{align*}
\text{Erro de variância} &= G(q, \hat{\theta}_N)-G(q, \theta^*)\\ 
\text{Erro de polarização}  &= G(q, \theta^*)-G_0(q) 
\end{align*}

O caso da parametrização independente em \eqref{eq:si_par_estim_independent_param} cobre o caso do {\it{Output Error}}
apresentado na Tabela \ref{table:si_modeling_models} além de modelos com estruturas fixas para o filtro do ruido
$H(q,\theta)$ como em:

\begin{equation}
y(t)=G(q,\theta)u(t)+H_*(q)e(t)
\nonumber
\end{equation}
onde $H_*(q)$ é um filtro do ruído fixo, onde $\mathcal{S} \notin \mathcal{M}$.

Este modelo também cobre o caso da estrutura de modelos Box-Jenkins (Tabela \ref{table:si_modeling_models}).
Consequentemente esta estrutura de modelos tem a vantagem de que a função de transferência $G(q, \theta)$ pode ser
consistentemente estimada, mesmo quando o modelo do filtro do ruído é muito simples para admitir-se uma completa
descrição do sistema \cite{ljung}.

%===============================================================================
\subsection{Distribuição assintótica das estimativas dos parâmetros}
\label{sec:si_par_estim_uncertanties_covariance}
%===============================================================================

Um meio comum de medir a qualidade das estimativas é estudar suas propriedades assintóticas. Quando o valor
da quantidade de dados $N$ cresce muito, a estimativa pertencerá a alguma distribuição. As propriedades desta
irão determinar a qualidade das estimativas obtidas \cite{jansson}.

Uma vez tendo entendido as propriedades de convergência de $\hat{\theta}_N$, a pergunta que surge é quão rápido esta
estimativa chega ao limite. Se $\theta^*$ denota o limite, estamos interessados na variável $\hat{\theta}_N -\theta^*$
que é uma variável randômica, e seu ``tamanho'' pode ser caracterizado pela sua matriz de covariância, ou pela sua
distribuição probabilística. $\hat{\theta}_N -\theta^*$ tipicamente decai a uma taxa $1/\sqrt{N}$. Então a distribuição
da variável randômica:

\begin{equation}
\sqrt{N}(\hat{\theta}_N -\theta^*)
\nonumber
\end{equation}
irá convergir para uma distribuição bem definida Gausiana \cite{ljung}.

Assume-se que dentro do conjunto $D_{\mathcal{M}}$ exista apenas um ponto de convergência $\theta^*$. Em
\eqref{eq:si_par_estim_under_theta} foi apresentado que $\hat{\theta}_N \to \theta^*$ quando $N \to \infty$ e considerando que :

\begin{equation}
\sqrt{N}D_N \to 0, \;\text{quando } N\to \infty
\nonumber
\end{equation}
onde $D_N$ pode ser definido como:

\begin{equation}
D_N=E\left [ \frac{1}{N}\sum_{t=1}^{N}\left [ \psi (t, \theta^*)\varepsilon (t, \theta^*)-E\psi(t, \theta^*)
\varepsilon(t, \theta^*) \right ] \right ]
\end{equation}
e

\begin{equation}
\psi(t, \theta^*) = -\frac{\mathrm{d}}{\mathrm{d}\theta} \varepsilon (t,\theta) \mid_{\theta=\theta^*} =
\frac{\mathrm{d}}{\mathrm{d} \theta}\hat{y}(t\mid \theta)\mid_{\theta=\theta^*}
\nonumber
\end{equation}

Então tem-se que:

\begin{equation}
\sqrt{N}(\hat{\theta}_N-\theta^*) \to \mathcal{N}(0,P(\theta)) \;\; \text{quando} \;\; N\to \infty 
\end{equation}
onde $\mathcal{N}(0,P(\theta))$ denota uma distribuição normal e $P(\theta)$ é dado por:

\begin{equation}
P(\theta^*)\overset{\underset{\mathrm{\Delta}}{\,}}{=}\lim_{N\to
\infty}N\;E(\hat{\theta}_N-\theta^*)(\hat{\theta}_N-\theta^*)^T
\label{eq:si_par_estim_P_theta}
\end{equation}

Para o caso onde $\mathcal{S} \in \mathcal{M}$ e assumindo ainda que $\varepsilon(t,\theta_0)=e_0(t)$ é uma sequência
randômica independente de média zero e variância $\lambda_0$. Desta forma tem-se

\begin{equation}
P(\theta)=\lambda_0\left [ E\;\psi (t,\theta_0)\psi^T(t,\theta_0)  \right ]^{-1}
\label{eq:si_par_estim_P_theta_match}
\end{equation}

Assim quando o sistema real pertence à classe  de modelos escolhida a estimativa converge para $\theta_0$ e a covariância da
estimativa é dada por \cite{ljung}:

\begin{equation}
\text{Cov}\hat{\theta}_N \sim \frac{1}{N}P(\theta)
\end{equation}

Não é apenas o tamanho ($N$) do experimento que irá influenciar na qualidade da estimativa. Introduz-se o espectro:

\begin{equation}
\Phi_{\chi 0}=\begin{bmatrix}
\Phi_u & \Phi_{ue}\\ 
\Phi_{ue} & \lambda_0
\end{bmatrix}
\label{eq:si_par_estim_cov_spectrum}
\end{equation}
onde $\Phi_u$ é o espectro da entrada relativo ao sinal $u(t)$, $\Phi_{ue}$ é o espectro cruzado entre os sinais $u(t)$
e $e(t)$. A distribuição da frequência do espectro $\Phi_{\chi 0}$ também influenciará na qualidade da
estimativa, como pode ser visto no Lemma \ref{lemma:si_par_estim_covariance}.

\begin{lemma}
A inversa da matriz de covariância, $P^{-1}(\theta_0)$, é uma função linear do espectro $\Phi_{\chi 0}$ dada por:

\begin{equation}
P^{-1}(\theta_0)=\frac{1}{2\pi \lambda_0}\int_{-\pi}^{\pi}\mathcal{F}(\theta_0)
\Phi_{\chi_0}(\theta_0)\mathcal{F}^*(\theta_0) d \omega
\label{eq:si_par_estim_cov_lemma}
\end{equation}
onde $\mathcal{F}(q, \theta_0)=\left [ \mathcal{F}_u(q, \theta_0) \;\;\;\mathcal{F}_e(q, \theta_0) \right ]$ e:

\begin{equation}
\mathcal{F}_u(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} G(\theta_0)}{\mathrm{d} \theta} 
\nonumber
\end{equation}

\begin{equation}
\mathcal{F}_e(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} H(\theta_0)}{\mathrm{d} \theta}
\nonumber
\end{equation}
\label{lemma:si_par_estim_covariance}
\end{lemma}

Como $P(\theta)$ é a medida do tamanho do erro nos parâmetros, o Lemma \ref{lemma:si_par_estim_covariance} mostra
como este erro é relacionado com o espectro $\Phi_{\chi 0}$ e especialmente com o espectro de entrada $\Phi_{u}$ e 
o espectro cruzado $\Phi_{ue}$. É interessante perceber que as únicas informações capazes de moldar a covariância
$P(\theta)$ são exatamente os espectro de entrada e cruzado, todos os outros fatores são dependentes do desconhecido
sistema real. Outro ponto a observar é que o espectro cruzado $\Phi_{ue}$ é zero quando o sistema esta operando em malha
aberta \cite{jansson}.

%===============================================================================
\subsubsection{Margens de confiança das estimativas}
\label{sec:si_par_estim_uncertanties_confidence_bounds}
%===============================================================================

A partir da distribuição assintótica normal em \eqref{eq:si_par_estim_P_theta} segue:

\begin{equation}
(\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0)\rightarrow \chi^2(n)\;\; \text{quando}\; N\to \infty
\nonumber
\end{equation}

Com $P_N=P/N$ e onde:

\begin{equation}
U_\theta=\left \{ \theta \mid (\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0) \le \chi^2_{\alpha}(n)
\right \}
\label{eq:si_par_estim_confidence_region}
\end{equation}
é a região de confiança, que assintoticamente inclui $\theta_0$ com probabilidade $\alpha$. Desta forma tem-se o
valor real $\theta_0$ com probabilidade $\alpha$ estará contido no elipsóide $U_\theta$ \cite{jansson}.

Na Figura \ref{fig:si_covar_elipse} é apresentado um exemplo de região de confiança para um $\chi$ de 99\%. O ponto
em destaque é a média de todas as estimativas e está localizado no centro da região de confiança.

\begin{figure}[htbp]
	\center
	\includegraphics[width=0.8\columnwidth]{figures/si_covar_elipse.eps}
	\caption{Estimativas de um sistema e a elipse representando a região de confiança para um $\chi$ de 99\%. No centro da
	elipse destaca-se a média de todas as estimativas.}
	\label{fig:si_covar_elipse}
\end{figure}


