%===============================================================================
\section{Definições}
\label{sec:sys_ident_intro}
%===============================================================================
% mains idea of this section: define witch systems willl we be managing: LTI TD SISO
% what are the 3 main elements of system identification: real system, model, an some criteria
% prediction error model comes here.

Neste capítulo tem-se o interesse de estudar sistemas lineares invariantes no tempo (LTI - {\it{linear time
invariant}}) de tempo discreto SISO ({\it{Single input single output}}). Este tipo de sistema pode ser representado
como:

\begin{equation}
y(t)=G_0(z)u(t)+H_0(z)e(t)
\label{eq:si_intro_system}
\end{equation}
onde $G_0(z)$ é a função de transferência que descreve o comportamento da planta real, $H_0(z)$ é a função de
transferência que atua sobre o ruído branco $e(t)$, $u(t)$ é o sinal de entrada aplicado sobre a planta e $y(t)$ o sinal
de saída, ambos medidos e conhecidos.

Para identificar o sistema apresentado em \eqref{eq:si_intro_system} utiliza-se uma família de modelos parametrizada
por um vetor $\theta \in \mathbb{R}^d$. Desta forma o sistema a ser identificado pode ser representado como:

\begin{equation}
y(t)=G(z, \theta)u(t)+H(z, \theta)e(t)
\label{eq:si_intro_model}
\end{equation}
onde as funções de transferência $G(z, \theta)$ e $H(z, \theta)$ podem ser definidas como:

\begin{equation}
G(z, \theta)=\sum_{k=1}^{\infty}g(k, \theta)z^{-k}
\nonumber
\end{equation}

\begin{equation}
H(z, \theta)=1+\sum_{k=1}^{\infty}h(k, \theta)z^{-k}
\nonumber
\end{equation}

O objetivo da identificação de sistemas é encontrar um valor para o vetor $\theta$ que faça com que $G(z,\theta)$ e
$H(z,\theta)$  sejam o mais próximos possíveis de $G_0(z)$ e $H_0(z)$. O vetor $\theta$ que faz com que estas duas
funções de transferências sejam iguais é denotado por $\theta_0$:

\begin{equation}
G(z, \theta_0)=G_0(z)
\label{eq:si_intro_true_theta}
\end{equation}

Outra definição que acompanha o que aqui será abordado é a definição de sinal quasi-estacionário:
Um sinal é dito quasi-estacionário se a média e a autocorrelação do mesmo convergem para um valor
finito quando o tamanho da amostra cresce, conforme definição a seguir:

\begin{defn}
\cite{ljung}
Um processo quasi-estacionário pode ser definido como:
\begin{itemize}
	\item $\bar{E}\left [ s(t) \right ] = m_s(t), \;\; \left | m_s \right | \le C, \;\forall t$;
	\item $\bar{E}\left [ s(t)s(r) \right ] = \mid R_s(t,s) \mid \le C, \;\forall t,r$;
	\item $\lim_{N \to \infty}\frac{1}{N}\sum_{t=1}^{N}R_s(t, t-\tau)=R_s(\tau), \; \forall \tau$,
\end{itemize}
onde $m_s(t)$ é o valor médio de $s(t)$ e $R_s(t,r)$ é a covariância de $s$ nos instantes $t$ e $r$.
\end{defn}

$\bar{E}[\cdot]$ é  denotado esperança e pode ser definido por: \cite{ljung}

\begin{equation}
\bar{E}\left [ f(t) \right ]\overset{\underset{\mathrm{\Delta}}{\,}}{=}\lim_{N \to \infty}
\frac{1}{N}\sum_{t-1}^{N}E\left [ f(t) \right ]
\label{eq:si_intro_E}
\end{equation}


%===============================================================================
\subsection{Elementos da identificação}
\label{sec:sys_ident_elements}
%===============================================================================

A identificação de sistemas é dependente de três componentes básicas. Primeiro delas é o sistema real, definido pelo
símbolo $\mathcal{S}$ e que pode ser definido a partir de \eqref{eq:si_intro_system} como abaixo:

\begin{equation}
\mathcal{S}:\;\; y(t)=G_0(z)u(t)+H_0(z)e(t)
\label{eq:si_intro_true_system}
\end{equation}

Segundo componente é a classe de modelos escolhida, denotada por $\mathcal{M}$, onde:

\begin{equation}
\mathcal{M}: \;\;\left \{ G(z, \theta), H(z, \theta) | \theta \in D_{\mathcal{M}} \right \}
\label{eq:si_intro_model}
\end{equation}

O terceiro componente é o critério de escolha que diz qual modelo, dentro da classe é melhor que outro. Em
outras palavras, qual modelo melhor representa o sistema real $\mathcal{S}$ de acordo com o critério escolhido.
Dentre os mais diversos critérios possíveis o mais utilizado é o de erro de predição, descrito a seguir. 

Preditores são equações que predizem qual será o próximo valor de saída do sistema baseado na classe do modelo e nos
valores de dados coletados até aquele instante. Os sinais $y(t)$ e $u(t)$ são os sinais da saída e entrada medidas do
sistema real enquanto que o sinal $\hat{y}(t, \theta)$ é o sinal de saída do preditor. A diferença entre o valor do
preditor e o valor do sistema real é conhecida como erro de predição:

\begin{equation}
\varepsilon(t, \theta)=y(t) - \hat{y}(t, \theta)	
\label{eq:si_estim_prediction_error}
\end{equation}

A Figura (\ref{fig:si_estim_pem}) apresenta um diagrama de blocos de como é organizado o método do erro de
predição. Existe um processo o qual se deseja identificar, para isso escolhe-se uma classe de modelos onde os
parâmetros a serem ajustados são representados por $\theta$. O preditor é então ajustado, baseado nas diferentes
possibilidades de escolha de $\theta$. Este ajuste é feito a partir do erro entre o sistema real $y(t)$ e a saída do
preditor $\hat{y}(t, \theta)$.

\begin{figure}[htbp]
\center
%\scalebox{1} % Change this value to rescale the drawing.
%{
\begin{pspicture}(0,-2.6)(10.859062,2.6)
\psframe[linewidth=0.04,dimen=outer](4.2,2.6)(1.8,1.4)
\psframe[linewidth=0.04,dimen=outer](5.4,0.2)(3.0,-1.0)
\pscircle[linewidth=0.04,dimen=outer](8.2,1.0){0.4}
\psframe[linewidth=0.04,dimen=outer](8.6,-1.4)(5.6,-2.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(1.8,2.0)(0.0,2.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(4.2,2.0)(10.0,2.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(8.2,1.4)(8.2,2.0)
\psline[linewidth=0.04cm](5.4,-0.4)(8.2,-0.4)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(8.2,-0.4)(8.2,0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(8.6,-2.0)(9.2,-2.0)
\psline[linewidth=0.04cm](9.2,-2.0)(9.2,1.0)
\psline[linewidth=0.04cm](9.2,1.0)(8.6,1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(4.0,-1.0)(4.0,-2.0)
\psline[linewidth=0.04cm](5.6,-2.0)(4.0,-2.0)
\psline[linewidth=0.04cm](1.2,2.0)(1.2,-0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(3.0,-0.2)(2.0,-0.2)
\psline[linewidth=0.04cm](2.0,-0.2)(2.0,0.8)
\psline[linewidth=0.04cm](2.0,0.8)(5.0,0.8)
\psline[linewidth=0.04cm](5.0,0.8)(5.0,2.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(3.0,-0.6)(1.2,-0.6)
\usefont{T1}{ptm}{m}{n}
\rput(2.9426563,2.11){Processo}
\usefont{T1}{ptm}{m}{n}
\rput(4.2964063,-0.29){Preditor}
\usefont{T1}{ptm}{m}{n}
\rput(7.1589065,-1.65){Algoritmo}
\usefont{T1}{ptm}{m}{n}
\rput(7.177656,-1.95){para minimizar}
\usefont{T1}{ptm}{m}{n}
\rput(7.140625,-2.33){f($\varepsilon(t,\theta)$)}
\usefont{T1}{ptm}{m}{n}
\rput(8.194531,0.99){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(7.9126563,1.51){+}
\usefont{T1}{ptm}{m}{n}
\rput(7.847344,0.51){-}
\usefont{T1}{ptm}{m}{n}
\rput(9.694531,1.31){$\varepsilon(t,\theta)$}
\usefont{T1}{ptm}{m}{n}
\rput(6.514531,2.31){$y(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(0.71453124,2.31){$u(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(7.114531,-0.09){$\hat{y}(t, \theta)$}
\usefont{T1}{ptm}{m}{n}
\rput(4.2632813,-0.69){Ajustável ($\theta$)}
\end{pspicture} 
%}
\caption{Diagrama de blocos para o método do erro de predição}
\label{fig:si_estim_pem}
\end{figure}

Considerando o sistema linear:

\begin{equation}
y(t)=G(z, \theta)u(t)+H(z, \theta)e(t)
\label{eq:si_par_estim_system}
\end{equation}

Assumindo que $G(0, \theta)=0$, $H(0,\theta)=1$ e que $H^{-1}(z, \theta)$ e $H^{-1}(z,\theta)
G(z,\theta)$ são assintoticamente estáveis, se $u(t)$ e $e(p)$ para $p<t$ não forem correlacionados,
então o preditor ótimo pode ser apresentado como: \cite{system_identification}

\begin{equation}
\hat{y}(t|t-1, \theta)= H^{-1}(z, \theta)G(z, \theta)u(t)+\left \{ 1-H^{-1}(z, \theta) \right \}y(t)
\label{eq:si_par_estim_predictor}
\end{equation}

\begin{equation}
\varepsilon (t,\theta)=H^{-1}(z, \theta)\left \{ y(t)-G(z, \theta)u(t) \right \}
\nonumber
\end{equation}

De posse da definição do erro de predição $\varepsilon (t,\theta)$ é conveniente introduzir o  critério mais utilizado
para elencar qual é o melhor modelo dentro da classe dentre os infinitos possíveis. Este critério será utilizado no
decorrer do texto e é a base para a maioria dos métodos de identificação usualmente utilizados.

\begin{equation}
V(\theta)=\frac{1}{N}\sum_{t=1}^{N}\frac{1}{2} \varepsilon ^2(t, \theta)
\label{eq:si_obj_etim_lsm_v}
\end{equation}

Este critério é também conhecido como função custo da identificação de sistemas por apresentar o valor do erro
quadrático entre os dados obtidos do sistema real $\mathcal{S}$ e os dados obtidos por meio do modelo estimado.
