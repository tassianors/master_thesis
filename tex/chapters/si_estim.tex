%===============================================================================
\section{Estimativa de parâmetros}
\label{sec:sys_ident_parameters_estimation}
%===============================================================================

A estimativa dos parâmetros dos modelos dependem de vários fatores, até agora foi apresentado
a importância dos dados coletados (Seção (\ref{sec:sys_ident_data_acquisition})) e da 
escolha do modelo (Seção (\ref{sec:sys_ident_modelling_choosing})). Nesta seção serão 
apresentados algumas formas para a estimativa dos parâmetros, bem como algumas de suas
características probabilísticas. 

A identificação é baseada em um conjunto de dados coletados do sistema, um modelo para caracteriza-lo.
Um preditor é uma equação que tenta prever o próximo valor do sistema baseado nos dados passados deste.
Com o preditor atinge-se um conjunto de dados que deve ser muito próximo aos dados verdadeiros
coletados do sistema. Escolhe-se então um método para minimização do erro existente entre
os dados coletados e os dados calculados pelo preditor.

Por fim será apresentado algumas das características para a estimação quando alguns 
requisitos para a identificação não não atingidos, como por exemplo quando o modelo
escolhido não consegue representar o sistema, ou quando o dados de entrada não são 
suficientemente informativos. Nestas situações teremos erros na estimativa dos parâmetros.
Erros diferentes que serão abordados na seção (\ref{sec:si_par_estim_uncertanties}).


%===============================================================================
\subsection{Preditores}
\label{sec:si_par_estim_preditors}
% ljung pg 68
%===============================================================================

Preditores são equações que tentam prever qual será o próximo valor de saída do sistema
baseado no modelo deste e nos valores de dados coletados até aquele momento.
Considere o sistema apresentado em (\ref{eq:si_modeling_lti}). Assume-se que os
sinais $y(p)$ e $u(p)$ são conhecidos para $p \le t-1$. A partir de (\ref{eq:si_par_estim_vs})
tem-se que até $\upsilon (p)$ é definido. O objetivo então é prever $y(t)$ como
em (\ref{eq:si_par_estim_yt}).

\begin{equation}
\upsilon (p)=y(p) -G(q)u(p)
\label{eq:si_par_estim_vs}
\end{equation}

\begin{equation}
y(t)=G(q)u(t)+\upsilon (t)
\label{eq:si_par_estim_yt}
\end{equation}

Fazendo-se as substituições necessárias chega-se ao estimador (\ref{eq:si_par_estim_predictor})
onde enfatiza-se a dependência com o parâmetro $\theta$ \cite{ljung}. Este preditor também
é conhecido como preditor ótimo, sendo utilizado em diversos métodos de identificação de
sistemas.

\begin{equation}
\hat{y}(t|\theta)=H^{-1}(q,\theta)G(q,\theta)u(t)+\left [ 1- H^{-1}(q,\theta)\right ]y(t)
\label{eq:si_par_estim_predictor}
\end{equation}

O erro de predição é a diferença entre os valores atingido pelo preditor e os valores 
reais coletados do sistema como apresentado em \eqref{eq:si_par_estim_err_predic}.
Este erro é amplamente utilizado para determinar a qualidade da estimativa que se 
encontra. Como será visto a seguir.

\begin{equation}
\varepsilon (t| \theta)=y(t)-\hat{y}(t|\theta)
\label{eq:si_par_estim_err_predic}
\end{equation}

Onde $y(t)$ são os dados coletados do sistema e $\hat{y}(t|\theta)$ os dados provenientes da 
estimativa. A equação do erro (\eqref{eq:si_par_estim_err_predic} pode ser reescrita como:

\begin{equation}
\varepsilon (t,\theta)=\frac{D(q)}{C(q)}\left [ A(q)y(t) - \frac{B(q)}{F(q)}u(t)\right ]
\nonumber
\end{equation}

Definimos então duas variáveis auxiliares \eqref{eq:si_estim_predict_w} e \eqref{eq:si_estim_predict_v}

\begin{equation}
w(t,\theta)=\frac{B(q)}{F(q)}u(t)
\label{eq:si_estim_predict_w}
\end{equation}

\begin{equation}
\upsilon (t,\theta)=A(q)y(t)-w(t,\theta)
\label{eq:si_estim_predict_v}
\end{equation}

Então:

\begin{equation}
\varepsilon (t,\theta)=y(t)-\hat{y}(t|\theta)=\frac{D(q)}{C(q)}\upsilon (t,\theta)
\nonumber
\end{equation}

Desta forma introduz-se o vetor de estados \eqref{eq:si_estim_predic_state}.\cite{ljung}

\begin{equation}
\begin{matrix}
\varphi (t,\theta)= & [ -y(t-1), ... , -y(t-n_a), u(t-1), ..., u(t-n_b),\\ 
 & -w(t-1, \theta),..., -w(t-n_f, \theta), \varepsilon (t-1, \theta), ...,\varepsilon (t-n_c, \theta),\\ 
 & \upsilon (t-1, \theta), ...,\upsilon (t-n_d, \theta) ]^T
\end{matrix}
\label{eq:si_estim_predic_state}
\end{equation}

E o vetor de parâmetros \eqref{eq:si_estim_predict_theta}:

\begin{equation}
\theta = \left [ a_1 \; ... \; a_{na} \; b_1 \; ... \; b_{nb} \; f_1 \; ... \; f_{nf} \; c_1 \; ... \; c_{nc} \; d_1 \; ... \; d_{nd}\right ]^T
\label{eq:si_estim_predict_theta}
\end{equation}

Chega-se então a seguinte equação para o erro de predição:

\begin{equation}
\varepsilon (t,\theta)=y(t)-\theta^T\varphi (t,\theta)
\label{eq:si_estim_predict_erro}
\end{equation}

Onde:

\begin{equation}
\hat{y}(t| \theta)=\theta^T\varphi (t,\theta)=\varphi ^T(t,\theta)\theta
\nonumber
\end{equation}

%===============================================================================
\subsection{Método dos mínimos quadrados}
\label{sec:si_par_estim_lsm}
%===============================================================================

Existem diversos métodos para a estimativa de parâmetros. O mais conhecido, remete
ao ano de 1809 utilizado por Gauss para determinação da orbita dos planetas. 
\cite{system_identification}. Método este chamado de Método dos mínimos quadrados (MMQ).

A regressão linear é o tipo mais simples de modelo paramétrico. A estrutura do modelo
pode ser descrita como em (\ref{eq:si_lsm_single_var}).

\begin{equation}
y(t)=\varphi ^T(t)\theta
\label{eq:si_lsm_single_var}
\end{equation}

Onde $y(t)$ é chamada de {\it{variável regredida}} e é medida do processo.
$\varphi (t)$ é comumente chamado de {\it{variável de regressão}} e $\theta$ é o vetor de
parâmetros.

O modelo apresentado em (\ref{eq:si_lsm_single_var}) é facilmente estendido para o modelo
multivariáveis (\ref{eq:si_lsm_multi_var}).

\begin{equation}
y(t)=\Phi ^T(t)\theta
\label{eq:si_lsm_multi_var}
\end{equation}

Onde $y(t)$ é um vetor de $p$ posições, $\Phi(t)$ uma matriz $n \times p$ e $\theta$ é um 
vetor de $N$ posições.

A ideia do método é encontrar uma estimativa $\hat{\theta}$ dos parâmetros de $\theta$ a partir de medidas
de $y(1),\varphi(1),\cdots,y(N),\varphi(N)$. 

A partir de (\ref{eq:si_par_estim_err_predic}) e (\ref{eq:si_lsm_single_var}) temos 

\begin{equation}
\varepsilon (t)=y(t)-\varphi ^T(t)\theta
\nonumber
\end{equation}

A {\it{estimativa dos mínimos quadrados}} de $\theta$ é definido como o vetor $\hat{\theta}$ 
que minimiza a função custo (\ref{eq:si_par_etim_lsm_v}). Este é o critério de minimização do 
erro de predição para o método dos mínimos quadrados.

\begin{equation}
V(\theta)=\frac{1}{2}\sum_{t=1}^{N}\varepsilon ^2(t)=\frac{1}{2}\varepsilon^T\varepsilon=\frac{1}{2}\left \| \varepsilon \right \|
\label{eq:si_par_etim_lsm_v}
\end{equation}

O valor de $\hat{\theta}$ que minimiza (\ref{eq:si_lsm_single_var}) é dado por:

\begin{equation}
\hat{\theta}=(\varphi ^T \varphi )^{-1}\varphi  ^T y
\label{eq:si_par_etim_lsm_theta}
\end{equation}

O mínimo da função custo fica como em:

\begin{equation}
\underset{\theta}{min}\;V(\theta)=V(\hat{\theta})=\frac{1}{2}\left [ y^Ty-y^T\varphi (\varphi ^T \varphi )^{-1}\varphi ^T y \right ]
\end{equation}

%===============================================================================
\subsection{Método das variáveis instrumentais}
\label{sec:si_par_estim_iv}
%===============================================================================
% bibliografia principal: aguirre e ljung

O método dos mínimos quadrados apresentados na seção (\ref{sec:si_par_estim_lsm}) é
simples de ser aplicado, mas tem o inconveniente de que para que não existam erros de 
polarização na estimativa, a variável de regressão $\phi(t)$ não pode estar
correlacionada com o distúrbio estocástico $\nu(t)$. Assume-se que o sistema real é dado
por:

\begin{equation}
y(t)=\varphi^T(t)\theta_0+\nu(t)
\label{eq:si_par_etim_iv_true_sys}
\end{equation}

No método dos mínimos quadrados $\varphi(t)$ depende da saída e implicitamente dos valores passados
de $\nu(t)$, desta forma \eqref{eq:si_par_etim_iv_estim} seria bem restritivo, mas de fato o que
pode ser demonstrado é que \eqref{eq:si_par_etim_iv_estim} é satisfeito somente se $\nu(t)$
for ruido branco. Esta desvantagem do método dos mínimos quadrados pode ser visto como uma
vantagem para a introdução do método de variáveis instrumentais. \cite{system_identification}

\begin{equation}
E(\varphi(t) \; \nu(t)) = 0
\label{eq:si_par_etim_iv_estim}
\end{equation}

Assume-se que $Z(t)$ é uma matriz $n\times n$ que possuem sinais não correlacionados com o 
distúrbio $\nu(t)$. O parâmetro $\theta$ deve obedecer a restrição da equação \eqref{eq:si_par_estim_iv_theta}:

\begin{equation}
\frac{1}{N}\sum_{t=1}^{N}Z(t)\varepsilon (t)=\frac{1}{N}\sum_{t=1}^{N}Z(t)\left [ y(t)-\varphi^T(t)\theta \right ]= 0
\label{eq:si_par_estim_iv_theta}
\end{equation}

Se a dimensão da matriz $Z(t)$ for a mesma dimensão de $\theta$ temos o estimados dos método de 
variáveis instrumentais \eqref{eq:si_par_estim_iv}:

\begin{equation}
\hat{\theta}=\left [ \sum_{t=1}^{N}Z(t)\varphi^T(t) \right ]^{-1}\left [  \sum_{t=1}^{N}Z(t)y(t) \right ]
\label{eq:si_par_estim_iv}
\end{equation}

Os elementos da matriz $Z(t)$ são normalmente chamados de instrumentos. O estimador das variáveis instrumentais 
é uma generalização do estimador dos mínimos quadrados, quando $Z(t)=\varphi(t)$. \cite{system_identification}

O estimador de variáveis instrumentais evita a polarização garantido  que o vetor de erro seja não correlacionado
com as variáveis instrumentais. Esta condição e menos restritiva que a condição dos mínimos quadrados para que
não haja erro de polarização \eqref{eq:si_par_etim_iv_estim}. O valor a ser pago por isso envolve: \cite{aguirre}

\begin{enumerate}[(I)]
\item Escolha das variáveis instrumentais.
\item O estimador resultante é assintoticamente não polarizado, ao invés de ser apenas não polarizado.
\end{enumerate}

Ao escolher as variáveis instrumentais é importante notar que a escolha não deve ser apenas para evitar 
a correlação entre o vetor de erro e os instrumentos. A razão para isso é que as variáveis instrumentais 
devem ser tão correlacionadas quanto possível com os regressores do modelo, caso contrário $Z(t)\varphi(t)^T$ 
seria próxima a singular e sua inversa muito mal condicionada. Portanto os instrumentos devem ser, idealmente, 
pouco correlacionados com o erro e muito correlacionados com os regressores do modelo. \cite{aguirre}


%===============================================================================
\subsection{Identificabilidade de sistemas}
\label{sec:si_par_estim_sys_ident}
%===============================================================================

A identificabilidade de um sistema pode ser definido pelo teorema (\ref{theorem:identificability}).

\begin{theorem} 
\cite{ljung}

Considerando a estrutura de modelo $\mathcal{M}$ correspondente a: 
\begin{equation}
A(q)y(t)=\frac{B(q)}{F(q)}u(t)+\frac{C(q)}{D(q)}e(t)
\label{eq:si_par_estim_sys_theorem}
\end{equation}

e com $\theta$ como em \eqref{eq:si_estim_predict_theta} sendo o coeficiente dos polinômios envolvidos.
Os graus dos polinômios são $n_a$, $n_b$ e assim por diante. A estrutura deste modelo 
é globalmente identificável se, e somente se, todos os itens de (i) até (vi) forem verdadeiros.

\begin{enumerate}[(i)]
\item Não existe fator comum para todos os $z^{na}A^*(z), z^{nb}B^*(z), and, \; z^{nc}C^*(z)$.
\item Não existe fator comum para $z^{nb}B^*(z), and, \; z^{nf}F^*(z)$.
\item Não existe fator comum para $z^{nc}C^*(z), and, \; z^{nd}D^*(z)$.
\item Se $n_a \geq 1$ então não pode existir fator comum entre $z^{nf}F^*(z), and, \; z^{nd}D^*(z)$.
\item Se $n_d \geq 1$ então não pode existir fator comum entre $z^{na}A^*(z), and, \; z^{nb}B^*(z)$.
\item Se $n_f \geq 1$ então não pode existir fator comum entre $z^{na}A^*(z), and, \; z^{nc}C^*(z)$.
\end{enumerate}

Os polinômios com $*$ correspondem a $\theta^*$
\label{theorem:identificability}
\end{theorem}

Note que as condições de (i) até (vi) são satisfeitas para o caso comum de \eqref{eq:si_par_estim_sys_theorem}.
Observe também que nenhuma das condições pode ser violada por qualquer {\it{especial}} valor de $\theta^*$, posto na
hiper-superfície de $\Re ^d$. Isso nos leva ao seguinte corolário:

\begin{cor} 
A estrutura de modelo apresentada em \eqref{eq:si_par_estim_sys_theorem} é globalmente identificável.
\label{corollary:identificability}
\end{cor} 

A partir do Corolário (\ref{corollary:identificability}) temos que a identificação de sistemas
utilizando os modelos descritos pela Tabela \ref{table:si_modeling_models} são globalmente identificáveis
se tivermos os dados de entrada suficientemente informativos.

%===============================================================================
\subsection{Incertezas nos parâmetros estimados}
\label{sec:si_par_estim_uncertanties}
%===============================================================================

O desejo principal para a estimativa de um sistema é que este não possua erros, ou ao menos que este erro, 
se existir, seja o menor possível. Para tanto é necessário caracterizar este tipo de informação do sistema.
Existem dois tipos principais de erro na estimativa de parâmetros. Um deles é o {\it{erro de variância}} e
outro é o {\it{erro de polarização}}.

Seja, $\theta^*$ o limite de convergência para a minimização do erro de predição:

\begin{equation}
\lim_{N \rightarrow \infty }\hat{\theta}_N = \theta^*
\label{eq:si_par_estim_under_theta}
\end{equation}

Sejam os vetores:

\begin{equation}
	Q(q)=\begin{bmatrix}
G_0(q) & H_0(q)
\end{bmatrix}^T
\nonumber
\end{equation}


\begin{equation}
\hat{Q}_N(q)=\begin{bmatrix}
G_0(q, \hat{\theta}_N) & H_0(q, \hat{\theta}_N)
\end{bmatrix}^T
\nonumber
\end{equation}

A qualidade da estimativa pode ser calculada em termos da diferença entre o processo real $Q_0(q)$ e o 
melhor sistema que o modelo pode atingir (quando a quantidade de dados é ilimitada ($N\rightarrow \infty$)) tem-se
então a estimativa do modelo $G_N(q)$.\cite{campestrini, solari}

\begin{equation}
\Delta Q_N(q) \equiv \hat{Q}_N-Q(q)
\label{eq:si_par_estim_under_diff}
\end{equation}

Define-se então:

\begin{equation}
Q^*(q) = \left [ G(q, \theta^*) \; H(q, \theta^*) \right ]^T
\label{eq:si_par_estim_under_q*}
\end{equation}

Entende-se por $Q^*(q)$ como a melhor aproximação que o método pode proporcionar (com $N \rightarrow \infty$) ou 
como sendo a média de todas as estimativas efetuadas.

Adicionando e subtraindo a \eqref{eq:si_par_estim_under_q*} na equação \eqref{eq:si_par_estim_under_diff}
chega-se a definição do erro de polarização e de variância \eqref{eq:si_par_estim_under_errors}.

\begin{equation}
\Delta Q_N(q) \equiv \underset{\text{Erro de variância}}{\underbrace{\hat{Q}_N-Q^*(q)}}+  \underset{\text{Erro de variância}}{\underbrace{Q^*(q)-Q(q)}}
\label{eq:si_par_estim_under_errors}
\end{equation}

A partir da \eqref{eq:si_par_estim_under_errors} compreende-se o erro de polarização como sendo a distância
entre a melhor aproximação possível e o valor real para o parâmetro. Já o erro de variância é a média que cada uma
das estimativas está distante do valor ótimo possível para a estimativa.

\begin{theorem}
Supondo um sistema linear $\mathcal{M}$ parametrizado com em \eqref{eq:si_par_estim_theorem_sys}
e  que os dados de entrada sejam suficientemente informativos.

\begin{equation}
\theta=\begin{bmatrix}
\rho \\ 
\eta 
\end{bmatrix},\;\;
G(q,\theta)=G(q,\rho), \;\;
H(q,\theta)=G(q,\eta)
\label{eq:si_par_estim_theorem_sys}
\end{equation}

Considerando que o sistema opera em malha aberta, ou seja:

\begin{equation}
u(t) \;e\; e_0(t)\text{ são independentes.}
\nonumber
\end{equation}

Seja:

\begin{equation}
\hat{\theta}_N=\begin{bmatrix}
\hat{\rho}_N \\ 
\hat{\eta}_N
\end{bmatrix}
\nonumber
\end{equation}

Obtido pelo método de predição apresentado %%achar a ref 
obtém-se:

\begin{equation}
\begin{matrix}
G(e^{j\omega},\hat{\theta}_N)\rightarrow G_0(e^{j\omega})\\ 
\text{quando:}\\
N\rightarrow \infty
\end{matrix}
\label{eq:si_par_estim_theorem_end}
\end{equation}
\end{theorem}

%===============================================================================
\subsubsection{Covariância nos parâmetros}
\label{sec:si_par_estim_uncertanties_covariance}
%===============================================================================

Um comum meio de medir a qualidade das estimativas é estudar suas propriedades assintóticas. Quando o valor
de $N$ (quantidade de dados) cresce muito, a estimativa pertencerá a alguma distribuição. As propriedades desta
irão determinar a qualidade das estimativas obtidas.\cite{jansson}

\begin{equation}
\begin{matrix}
\sqrt{N}(\hat{\theta}_N-\theta_0) \to \mathcal{N}(0,P) \;\; \text{quando} \;\; N\to \infty \\ \\
\lim_{N\to \infty}N\;E(\hat{\theta}_N-\theta_0)()\hat{\theta}_N-\theta_0)^T=P\\\\
P(\theta_0)=\lambda_0(E\left [ \psi (t,\theta_0)\psi^T(t,\theta_0) \right ])^{-1}\\ \\
\psi (t,\theta_0)=\frac{\partial }{\partial \theta}\hat{y}(t,\theta)\mid_{\theta=\theta_0}
\end{matrix}
\label{eq:si_par_estim_conv_def}
\end{equation}

Não é apenas o tamanho ($N$) do experimento que irá influenciar na qualidade da estimativa.
Na equação \eqref{eq:si_par_estim_cov_spectrum} apresenta-se o espectro onde $\Phi_u$ é o espectro da entrada $u(t)$, 
$\Phi_{ue}$ é o espectro cruzado entre a entrada e o erro $e_0$. Esta distribuição do espectro, $\Phi_{\chi 0}$
também influenciará na qualidade da estimativa, como pode ser visto no Lemma \ref{lemma:si_par_estim_covariance}.

\begin{equation}
\Phi_{\chi 0}=\begin{bmatrix}
\Phi_u & \Phi_{ue}\\ 
\Phi_{ue} & \lambda_0
\end{bmatrix}
\label{eq:si_par_estim_cov_spectrum}
\end{equation}

\begin{lemma}
A inversa da matriz de covariancia, $P^{-1}(\theta_0)$, é uma função linear do espectro $\Phi_{\chi 0}$ dado por
\eqref{eq:si_par_estim_cov_lemma}.

\begin{equation}
P^{-1}(\theta_0)=\frac{1}{2\pi \lambda_0}\int_{-\pi}^{\pi}\mathcal{F}(\theta_0)
\Phi_{\chi_0}(\theta_0)\mathcal{F}^*(\theta_0) d \omega
\label{eq:si_par_estim_cov_lemma}
\end{equation}

Onde $\mathcal{F}(q, \theta_0)=\left [ \mathcal{F}_u(q, \theta_0) \;\;\;\mathcal{F}_e(q, \theta_0) \right ]$ e:

\begin{equation}
\mathcal{F}_u(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} G(\theta_0)}{\mathrm{d} \theta} 
\nonumber
\end{equation}

\begin{equation}
\mathcal{F}_e(\theta_0) = H^{-1}(\theta_0)\frac{\mathrm{d} H(\theta_0)}{\mathrm{d} \theta}
\nonumber
\end{equation}
\label{lemma:si_par_estim_covariance}
\end{lemma}

Como $P$ é a medida do tamanho do erro nos parâmetros, o Lemma \ref{lemma:si_par_estim_covariance} mostra
como este erro é relacionado com o espectro $\Phi_{\chi 0}$. O espectro cruzado $\Phi_{ue}$ é zero quando 
o sistema esta operando em laço aberto. \cite{jansson}

%===============================================================================
\subsubsection{Margens de confiança das estimativas}
\label{sec:si_par_estim_uncertanties_confidence_bounds}
%===============================================================================

A partir da distribuição assintótica normal em \eqref{eq:si_par_estim_conv_def} segue:

\begin{equation}
(\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0)\rightarrow \chi^2(n)\;\; \text{quando}\; N\to \infty
\nonumber
\end{equation}

Com $P_N=P/N$ e onde \eqref{eq:si_par_estim_confidence_region} é a região de confiança, onde assintoticamente
inclui $\theta_0$ com probabilidade $\alpha$. Desta forma tem-se que as estimativas estarão centradas em $\theta_0$
e com probabilidade $\alpha$ estarão contidas em uma esfera definida por $P_N$ e $\chi_{\alpha}^2(N)$

\begin{equation}
U_\theta=\left \{ \theta \mid (\hat{\theta}_N-\theta_0)^T P^{-1}_{N}(\hat{\theta}_N-\theta_0) \le \chi^2_{\alpha}(n)  \right \}
\label{eq:si_par_estim_confidence_region}
\end{equation}

Na Figura (\ref{fig:si_covar_elipse}) é apresentado um exemplo de região de confiança para um $\chi$ de 95\%. O ponto em destaque
é a média de todas as estimativas e está localizado no centro da região de confiança.

\begin{figure}[htbp]
	\center
	\includegraphics[width=0.8\columnwidth]{figures/si_covar_elipse.eps}
	\caption{Estimativas de um sistema e a região de confiança para $\chi$ de 95\%}
	\label{fig:si_covar_elipse}
\end{figure}

