%===============================================================================
\section{Estruturas de modelos para sistemas não lineares}
\label{sec:nl_models}
%===============================================================================
% ideia aqui é colocar uma pequena introdução sobre modelos.. no mesmo estilo
% que foi para sistemas lineares.

Famílias ou conjuntos de modelos para sistemas não lineares podem ser divididos em dois grupos principais baseados na
natureza de suas não linearidades: não linearidades estáticas, onde a dinâmica do sistema pode ser bem caracterizada por um modelo
linear enquanto que a parte não linear está concentrada ou na entrada ou na saída do sistema de forma estática.
Estes sistemas são chamados de Wiener para não linearidades na saída do processo ou Hammerstein, quando a
não linearidade está na entrada do processo.

Para os demais casos, onde a não linearidade está na dinâmica do processo existem várias estruturas de modelos
que podem ser utilizados como será visto no decorrer deste capítulo. De forma geral todos estes conjuntos
possuem em comum a ideia da escolha de uma base que seja representativa e que possa reduzir a quantidade de
termos a ser identificado e com isso ter uma boa aproximação do sistema real com a menor quantidade de
parâmetros possível.

Percebe-se então que um mesmo sistema pode ser representado por diversos destes modelos, mas dependendo das
características deste sistema, uma das estruturas de modelos será mais adequada, por sua base possuir mais
afinidade em representar certos tipos de não linearidades. Entende-se por ``maior afinidade'' a característica de
conseguir representar o sistema com um número de parâmetros menor que outro modelo que use outra base.

Um dos passos mais desafiadores na construção de modelos não lineares é a escolha da estrutura de
modelo. Quando este modelo é não linear, existe uma grande quantidade de opções e com isso o perigo
de escolher um modelo desnecessariamente complexo é evidente. Este perigo baseia-se no {\it{princípio
da parcimônia}} que basicamente determina que o modelo deve ser o mais simples possível \cite{aguirre_maps}. Na Seção 
\ref{sec:si_exper_overfit} foram apresentadas algumas ideias relacionadas a este assunto.

Juntamente com a escolha do modelo para representar o sistema não linear, saber se este é identificável é de grande
importância. Em \cite{glad_ljung} é apresentado um procedimento para determinar a identificabilidade de sistemas não
lineares.

% ===============================================================================
\subsection{Modelos de Wiener e Hammerstein}
\label{sec:nl_models_wiener_hammerstein}
% Aguirre: 334
% ljung 143
%===============================================================================

Modelos de Wiener e Hammerstein são normalmente utilizados em situações onde a dinâmica do sistema pode ser bem descrita
por um sistema linear, mas existem algumas não linearidades estáticas atreladas à entrada e/ou à saída. Este será o
caso de atuadores não lineares com características como saturação, zona morta, entre outros. Um modelo com não
linearidades na entrada é chamado de {\it{modelo de Hammerstein}} e para não linearidades na saída chama-se {\it{modelo
de Wiener}} \cite{ljung}. Na Figura \ref{fig:nl_models_hammerstein_wiener} observa-se o diagrama de blocos para os
modelos de Hammerstein e Wiener.

\begin{figure}[htbp]
\center
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-1.6)(9.439062,1.6)
\usefont{T1}{ptm}{m}{n}
\rput(0.52453125,1.31){$u(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(3.9557812,1.31){$f(u(t))$}
\usefont{T1}{ptm}{m}{n}
\rput(7.554531,1.35){$y(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(5.8759375,1.15){Modelo}
\usefont{T1}{ptm}{m}{n}
\rput(5.7834377,0.75){Linear}
\usefont{T1}{ptm}{m}{n}
\rput(2.1957812,1.03){$f(\cdot)$}
\psframe[linewidth=0.04,dimen=outer](3.0,1.6)(1.2,0.4)
\psframe[linewidth=0.04,dimen=outer](6.8,1.6)(5.0,0.4)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.0,1.0)(5.0,1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.0,1.0)(1.2,1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.8,1.0)(8.0,1.0)
\usefont{T1}{ptm}{m}{n}
\rput(0.52453125,-0.69){$u(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(3.9357812,-0.69){$z(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(2.0759375,-0.85){Modelo}
\usefont{T1}{ptm}{m}{n}
\rput(1.9834375,-1.25){Linear}
\usefont{T1}{ptm}{m}{n}
\rput(5.9957814,-0.99){$f(\cdot)$}
\psframe[linewidth=0.04,dimen=outer](3.0,-0.4)(1.2,-1.6)
\psframe[linewidth=0.04,dimen=outer](6.8,-0.4)(5.0,-1.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.0,-1.0)(5.0,-1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.0,-1.0)(1.2,-1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.8,-1.0)(8.0,-1.0)
\usefont{T1}{ptm}{m}{n}
\rput(8.204532,-0.65){$y(t)=f(z(t))$}
\end{pspicture} 
}
\caption{Acima: modelo de Hammerstein. Abaixo: Modelo de Wiener.}
\label{fig:nl_models_hammerstein_wiener}
\end{figure}

%===============================================================================
\subsection{Série de Volterra}
\label{sec:nl_models_volterra}
% Aguirre 334
%===============================================================================

Um sistema não linear pode ser descrito pela série de Volterra:

\begin{equation}
y(t)=\sum_{j=1}^{\infty}\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}
h_j(\tau_1, ... ,\tau_j) \prod_{i=1}^{j}u(t-\tau_i)d\tau_i
\label{eq:nl_models_volterra}
\end{equation}
onde $h_j$ são generalizações não lineares da resposta ao impulso $h_1(t)$ . Quando $j=1$ a equação
de Volterra se reduz à integral de convolução. \cite{aguirre}

As expansões funcionais em séries de Volterra relacionam os sinais passados da entrada com o valor atual da saída do
sistema e isso inevitavelmente significa que que um conjunto tipicamente elevado de parâmetros seja necessário para
descrever até mesmo simples sistemas não lineares e consequentemente apenas poucas aplicações práticas foram apresentadas.
\cite{chen_billings1989}

%===============================================================================
\subsection{Redes Neurais}
\label{sec:nl_models_neurals}
%===============================================================================
As redes neurais artificiais são nada mais do que um tipo de base em particular que podem ser usadas para modelagem. As
redes neurais são compostas por camadas de neurônios interconectados. A saída de um neurônio com $n$ entradas é
apresentada na equação:

\begin{equation}
x=\emph{f}\left ( \sum_{j=1}^{n}\omega_j x_j +b \right )
\label{eq:nl_models_neural}
\end{equation}
onde $b$ é conhecido como {\it{bias}} e $\omega_j$ são constantes e $\emph{f}(\cdot)$ é chamada função de ativação.
$\omega_j$ e $b$ são os parametros a serem identificados. A função de ativação mais comum é: \cite{aguirre}

\begin{equation}
\emph{f}(q)=\frac{1}{1+e^{-q}}
\nonumber
\end{equation}

%===============================================================================
\subsubsection{Redes Neurais multi-camadas}
\label{sec:nl_models_neurals_multilayer}
%===============================================================================

Uma típica rede multi-camadas pode ser descrita como na Figura \ref{fig:nl_models_neural_multilayer}.  Na prática redes
neurais multi-camadas têm grande apelo no reconhecimento de padrões. Do ponto de vista teórico os sistemas de redes
multi-camadas podem ser considerados como mapas não lineares onde os elementos das matrizes de peso são os parâmetros.
\cite{narenda_parthasarathy}

\begin{figure}[htbp]
\center
% Generated with LaTeXDraw 2.0.8
% Sun Jul 29 14:39:55 BRT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{0.8} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.369375)(14.589063,3.329375)
\pscircle[linewidth=0.04,dimen=outer](4.77,2.729375){0.4}
\usefont{T1}{ppl}{m}{n}
\rput(4.804531,2.699375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](7.17,3.329375)(5.97,2.129375)
\usefont{T1}{ppl}{m}{n}
\rput(6.554531,2.719375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](4.77,0.529375){0.4}
\usefont{T1}{ppl}{m}{n}
\rput(4.804531,0.499375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](7.17,1.129375)(5.97,-0.070625)
\usefont{T1}{ppl}{m}{n}
\rput(6.554531,0.539375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](4.77,-1.870625){0.4}
\usefont{T1}{ppl}{m}{n}
\rput(4.804531,-1.900625){$\sum$}
\psframe[linewidth=0.04,dimen=outer](7.17,-1.270625)(5.97,-2.470625)
\usefont{T1}{ppl}{m}{n}
\rput(6.514531,-1.880625){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](10.37,2.729375){0.4}
\usefont{T1}{ppl}{m}{n}
\rput(10.384531,2.679375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](12.77,3.329375)(11.57,2.129375)
\usefont{T1}{ppl}{m}{n}
\rput(12.1145315,2.719375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](10.37,0.529375){0.4}
\usefont{T1}{ppl}{m}{n}
\rput(10.384531,0.479375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](12.77,1.129375)(11.57,-0.070625)
\usefont{T1}{ppl}{m}{n}
\rput(12.134531,0.499375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](10.37,-1.870625){0.4}
\usefont{T1}{ppl}{m}{n}
\rput(10.384531,-1.920625){$\sum$}
\psframe[linewidth=0.04,dimen=outer](12.77,-1.270625)(11.57,-2.470625)
\usefont{T1}{ppl}{m}{n}
\rput(12.134531,-1.920625){$\gamma$}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(5.17,2.729375)(5.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(5.17,0.529375)(5.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(5.17,-1.870625)(5.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(10.77,2.729375)(11.57,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(10.77,0.529375)(11.57,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(10.77,-1.870625)(11.57,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(12.77,2.729375)(13.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(12.77,0.529375)(13.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(12.77,-1.870625)(13.97,-1.870625)
\usefont{T1}{ppl}{m}{n}
\rput(13.674531,3.039375){$y_1$}
\usefont{T1}{ppl}{m}{n}
\rput(13.674531,0.839375){$y_2$}
\usefont{T1}{ppl}{m}{n}
\rput(13.674531,-1.560625){$y_n$}
\psdots[dotsize=0.12](6.77,-0.470625)
\psdots[dotsize=0.12](6.77,-0.670625)
\psdots[dotsize=0.12](6.77,-0.870625)
\psdots[dotsize=0.12](12.37,-0.470625)
\psdots[dotsize=0.12](12.37,-0.670625)
\psdots[dotsize=0.12](12.37,-0.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,2.729375)(9.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,2.729375)(9.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,2.729375)(9.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,0.529375)(9.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,-1.870625)(9.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,0.529375)(9.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,0.529375)(9.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,-1.870625)(9.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,-1.870625)(9.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7888126,2.7075653)(4.37,2.7075653)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7888126,2.7075653)(4.37,0.51799595)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7888126,2.7075653)(4.37,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,0.529375)(4.37,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,-1.850625)(4.37,-1.850625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,0.529375)(4.37,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,0.529375)(4.37,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,-1.850625)(4.37,0.5389402)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,-1.850625)(4.37,2.729375)
\usefont{T1}{ppl}{m}{n}
\rput(1.4745313,3.039375){$u_1$}
\usefont{T1}{ppl}{m}{n}
\rput(1.4745313,0.839375){$u_2$}
\usefont{T1}{ppl}{m}{n}
\rput(1.4745313,-1.560625){$u_n$}
\usefont{T1}{ppl}{m}{n}
\rput(1.4845313,-3.160625){$\text{Entrada}$}
\usefont{T1}{ppl}{m}{n}
\rput(6.7345314,-3.160625){$\text{Camada Oculta}$}
\usefont{T1}{ppl}{m}{n}
\rput(12.444531,-3.160625){$\text{Camada Saída}$}
\psdots[dotsize=0.12](1.57,-0.270625)
\psdots[dotsize=0.12](1.57,-0.470625)
\psdots[dotsize=0.12](1.57,-0.670625)
\psline[linewidth=0.04cm](1.77,2.729375)(1.17,2.729375)
\psline[linewidth=0.04cm](1.77,0.529375)(1.17,0.529375)
\psline[linewidth=0.04cm](1.77,-1.870625)(1.17,-1.870625)
\end{pspicture} 
}

\caption{Representação de uma rede neural multi-camadas.}
\label{fig:nl_models_neural_multilayer}
\end{figure}

A saída do sistema para uma rede multi-camadas pode ser descrita como:

\begin{equation}
y(t)=f_s\left \{ \sum_{i=1}^{m} \omega_i f_i \left ( \sum_{j=1}^{n}\omega_{ij} x_j + b_i \right ) +
b_s \right \}
\label{eq:nl_models_neural_multilayer}
\end{equation}
onde $f_s$ é a função de ativação do neurônio da camada de saída. Esta função não precisa ser
igual a $f_i$, $i=1, \ldots , m$ que por sua vez não precisam ser iguais entre si. As constantes $b_s$ são os termos
de polarização dos neurônios da camada de saída, $\omega_i$ são os pesos da saída de cada neurônio da
camada oculta e $\omega_{ij}$ são os pesos da entrada $j$, vista pelo $i-$ésimo neurônio da camada
oculta. \cite{aguirre}

%===============================================================================
\subsubsection{Redes Neurais recorrentes}
\label{sec:nl_models_neurals_multilayer}
%===============================================================================

Redes neurais recorrentes, trabalho introduzido por Hopfield em \cite{hopfield} provê uma
alternativa para o reconhecimento de padrões. O método proposto consiste em ter uma rede neural de
apenas uma camada adicionada de uma realimentação com um atraso de tempo como apresentado na Figura
\ref{fig:nl_models_neural_recurrent} \cite{narenda_parthasarathy}.

\begin{figure}[htbp]
\center
% Generated with LaTeXDraw 2.0.8
% Sun Jul 29 14:47:15 BRT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{0.8} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-5.4)(11.04,5.4)
\usefont{T1}{ppl}{m}{n}
\rput(8.175624,4.79){$\gamma$}
\usefont{T1}{ppl}{m}{n}
\rput(8.195624,3.23){$\gamma$}
\usefont{T1}{ppl}{m}{n}
\rput(8.195624,0.61){$\gamma$}
\usefont{T1}{ppl}{m}{n}
\rput(5.173437,-0.97){$q^{-1}$}
\pscircle[linewidth=0.04,dimen=outer](6.42,4.8){0.4}
\pscircle[linewidth=0.04,dimen=outer](6.42,0.6){0.4}
\pscircle[linewidth=0.04,dimen=outer](6.42,3.2){0.4}
\usefont{T1}{ppl}{m}{n}
\rput(6.395625,0.61){$\sum$}
\usefont{T1}{ppl}{m}{n}
\rput(6.395625,3.21){$\sum$}
\usefont{T1}{ppl}{m}{n}
\rput(6.395625,4.81){$\sum$}
\psdots[dotsize=0.12](5.82,4.8)
\psdots[dotsize=0.12](5.82,3.2)
\psdots[dotsize=0.12](5.82,0.6)
\psdots[dotsize=0.12](2.82,4.8)
\psdots[dotsize=0.12](2.82,3.2)
\psdots[dotsize=0.12](2.82,0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,4.8)(5.82,4.8)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,4.8)(5.82,3.1916058)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,4.8)(5.82,0.61817497)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,0.61817497)(5.82,4.8)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,3.1916058)(5.82,4.8)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,3.1916058)(5.82,3.1916058)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,3.1916058)(5.82,0.61817497)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,0.61817497)(5.82,3.1916058)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.82,0.61817497)(5.82,0.61817497)
\psline[linewidth=0.04cm](8.8,0.6)(9.42,0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(9.4,-1.0)(5.8,-1.0)
\psline[linewidth=0.04cm](4.62,-1.0)(1.6,-1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.62,0.6)(2.82,0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.82,0.6)(7.62,0.6)
\psline[linewidth=0.04cm](5.82,0.6)(6.0,0.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.82,3.2)(7.62,3.2)
\psline[linewidth=0.04cm](5.82,3.2)(6.02,3.2)
\psline[linewidth=0.04cm](5.82,4.8)(6.02,4.8)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.82,4.8)(7.62,4.8)
\psline[linewidth=0.04cm](8.8,3.2)(10.22,3.2)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(5.8,-2.6)(10.2,-2.6)
\psline[linewidth=0.04cm](8.8,4.8)(11.02,4.8)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(11.0,-4.8)(5.8,-4.8)
\psline[linewidth=0.04cm](4.62,-2.6)(0.8,-2.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.8,3.2)(2.82,3.2)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{<-}(2.82,4.8)(0.0,4.8)
\psline[linewidth=0.04cm](0.0,-4.8)(4.62,-4.8)
\psdots[dotsize=0.12](8.22,2.2)
\psdots[dotsize=0.12](8.22,2.0)
\psdots[dotsize=0.12](8.22,1.8)
\psdots[dotsize=0.12](5.22,-3.4)
\psdots[dotsize=0.12](5.22,-3.6)
\psdots[dotsize=0.12](5.22,-3.8)
\usefont{T1}{ppl}{m}{n}
\rput(9.864062,5.11){$x_1(t+1)$}
\usefont{T1}{ppl}{m}{n}
\rput(9.864062,3.51){$x_2(t+1)$}
\usefont{T1}{ppl}{m}{n}
\rput(9.864062,0.91){$x_n(t+1)$}
\usefont{T1}{ppl}{m}{n}
\rput(2.0040624,5.11){$x_1(t)$}
\usefont{T1}{ppl}{m}{n}
\rput(2.0040624,3.51){$x_2(t)$}
\usefont{T1}{ppl}{m}{n}
\rput(2.0040624,0.91){$x_n(t)$}
\usefont{T1}{ppl}{m}{n}
\rput(4.3840623,5.11){$\omega_i$}
\usefont{T1}{ppl}{m}{n}
\rput(5.153437,-2.61){$q^{-1}$}
\usefont{T1}{ppl}{m}{n}
\rput(5.153437,-4.79){$q^{-1}$}
\psframe[linewidth=0.04,dimen=outer](8.8,5.4)(7.6,4.2)
\psframe[linewidth=0.04,dimen=outer](8.8,3.8)(7.6,2.6)
\psframe[linewidth=0.04,dimen=outer](8.8,1.2)(7.6,0.0)
\psframe[linewidth=0.04,dimen=outer](5.8,-0.4)(4.6,-1.6)
\psframe[linewidth=0.04,dimen=outer](5.8,-2.0)(4.6,-3.2)
\psframe[linewidth=0.04,dimen=outer](5.8,-4.2)(4.6,-5.4)
\psline[linewidth=0.04cm](11.0,4.8)(11.0,-4.8)
\psline[linewidth=0.04cm](10.2,3.2)(10.2,-2.6)
\psline[linewidth=0.04cm](9.4,0.6)(9.4,-1.0)
\psline[linewidth=0.04cm](1.6,0.6)(1.6,-1.0)
\psline[linewidth=0.04cm](0.8,-2.6)(0.8,3.2)
\psline[linewidth=0.04cm](0.0,4.8)(0.0,-4.8)
\end{pspicture} 
}
\caption{Representação de redes neurais recorrentes. Onde $q^{-1}$ representa um atraso de tempo na realimentação do
sistema.}
\label{fig:nl_models_neural_recurrent}
\end{figure}

%===============================================================================
\subsection{Funções Radiais de Base}
\label{sec:nl_models_radiais}
% Aguirre 337
%===============================================================================

Funções radiais de base  ({\it{RBF - Radial basis functions}})  são uma tradicional técnica para
interpolação em espaços multidimensional \cite{chen_billings_narmax} e podem ser descritas como
mapeamentos do tipo:

\begin{equation}
f(y)=w_0+\sum_{i}w_i \phi (\left \| y-c_i \right \|)
\label{eq:nl_models_rbf}
\end{equation}
onde $y \in \mathbb{R}^{d_e}$ ($d_e$ é conhecido como dimensão de imersão),
$\left \| \cdot \right \|$ é a norma euclidiana, $w_i \in \mathbb{R}$ são pesos, 
$c_i \in \mathbb{R}^{d_e}$ são centros e $\phi(\cdot):\mathbb{R}^+ \to \mathbb{R}$ 
é uma função, normalmente escolhida a priori, como por exemplo: \cite{aguirre}

\begin{equation}
\phi(\left \| y-c_i \right \|)= exp\left ( -\frac{\left \| y-c_i \right \|^2}{\sigma_i^2} \right )
\nonumber
\end{equation}

Outras funções de base usadas são apresentadas na Tabela \ref{table:nl_models_rbf}.

\begin{table*}[htbp]
\begin{center}
\caption{Algumas funções Radiais de base comumente utilizadas.}
\label{table:nl_models_rbf}
\begin{tabular}{ll}
\hline
        Nome & Função   \\
\hline
        Multi quadrática inversa  & $\phi(r)=(r^2+\sigma ^2)^{-1/2}$ \\ 
        Linear                    & $\phi(r)=r$                      \\ 
        Cúbica                    & $\phi(r)=r^3$                    \\ 
        Multi-quadrática           & $\phi(r)=\sqrt{r^2+\sigma^2}$    \\ 
        {\it{Thin - plate spline}} & $\phi(r)=r^2\; \text{log}(r)$   \\ 
\hline
\end{tabular}
\end{center}
\end{table*}
onde $r=\left \| y-c_i \right \|$ e $\sigma$ definem a largura do chapéu no caso de
funções do tipo Gausianas e multi-quadráticas, como pode ser visto na Figura \ref{fig:nl_models_rbf}.

\begin{figure}[htbp]
	\center
	\includegraphics[width=0.8\columnwidth]{figures/nl_models_rbf.eps}
	\caption{Função multi-quadráticas inversa para alguns valores de $\sigma$.}
	\label{fig:nl_models_rbf}
\end{figure}

Este tipo de representação tem boas propriedades locais e pode ser interpretada como 
uma técnica de interpolação global. Funções radiais de base são casos particulares
de redes neurais, porém neste caso, lineares nos parâmetros $w_i$. \cite{aguirre} 

No contexto de identificação de sistemas é comum adicionar termos auto-regressivos
lineares, bem como termos de entrada à equação \eqref{eq:nl_models_rbf} resultando em:

\begin{equation}
y(k)=w_0 + \sum_{i}w_i \phi(\left \| \mathbf{y}(k-1)-c_i \right \|)+\sum_{i=1}^{n_y}a_i y(k-i)+\sum_{i=1}^{n_u}b_i u(k-i)+e(k)
\nonumber
\end{equation}
onde

\begin{equation}
\mathbf{y}(k-1)=\begin{bmatrix}y(k-1) & ... & y(k-n_y) & u(k-1) & ... & u(k-n_u)\end{bmatrix}
\nonumber
\end{equation}

%===============================================================================
\subsection{Modelos NARMAX}
\label{sec:nl_models_narmax}
% Aguirre 343
%===============================================================================

Os modelos {\it{NARX}} (do termo inglês {\it{nonlinear autoregressive model with exogenous variables}}) são modelos
discretos no tempo que caracterizam o valor da saída em função dos valores passados da entrada e da própria saída.
Algumas vezes, para evitar a polarização da estimativa dos parâmetros adiciona-se termos do modelo do ruído ao modelo
do sistema. Quando isso é feito o modelo passa a ser chamado de modelo {\it{NARMAX}} (do termo inglês {\it{nonlinear
autoregressive moving average model with exogenous variables}}), introduzido por \cite{leontaritis_billings1985}.

Este modelo provê uma representação unificada para a descrição de sistemas discretos não lineares:

\begin{equation}
y(t)=\theta^T\Phi_{nl}(y, u, e)
\label{eq:nlin_models_narmax_generic}
\end{equation}
onde $\Phi_{nl}(\cdot)$ denota um campo vetorial que depende dos valores passados de $y(t)$ e presente e passados de
$u(t)$ e $e(t)$; $\theta$ é o vetor de parâmetros a ser identificado.

As classes de modelos NARMAX podem então ser compreendidas como um conjunto ou somatório de funções não lineares, que
são parametrizados linearmente. Vale aqui esclarecer que a forma apresentada em \eqref{eq:nlin_models_narmax_generic}
não é uma regressão linear. Para uma regressão linear:

\begin{equation}
y(t)=\theta^T\phi(y(t), u(t))
\label{eq:nlin_models_narmax_reg}
\end{equation}
onde $\phi(\cdot)$ é dependente apenas dos sinais de entrada e saída do sistema, usualmente sendo uma matriz formada a
partir dos dados coletados do sistema. Diferentemente do equacionamento apresentado em
\eqref{eq:nlin_models_narmax_generic}, onde $\Phi_{nl}(\cdot)$ é dependente também do ruído. Outra diferença está no
fato de que em \eqref{eq:nlin_models_narmax_generic} é possível ter regressores não lineares. 

%===============================================================================
%===============================================================================
\subsection{Modelo polinomial}
\label{sec:nl_models_narmax_pol}
% Aguirre 343
%===============================================================================

Um modelo polinomial pode ser representado como 

\begin{equation}
y(t)=\sum_{i}c_i \prod_{j=1}^{n_y}y(t-j) \prod_{r=1}^{n_u}u(t-r) \prod_{q=0}^{n_e}e(t-q)
\label{eq:nl_model_narmax_pol}
\end{equation}
para este caso duas considerações serão levadas em conta:

\begin{enumerate}
\item O sistema tem um atraso puro de tempo $\tau _d$.
\item Nenhum termo cujo parâmetro tenha que ser estimado pode depender de $e(t)$.
\end{enumerate}

A segunda consideração implica em tornar a equação independente de $e(t)$. O que equivale a dizer
que $q=1$ em \eqref{eq:nl_model_narmax_pol} resultando em:

\begin{eqnarray}\nonumber
y(t) &=& F[ y(t-1), ..., y(t-n_y), u(t-\tau_d), ..., u(t-\tau_d-n_u+1),\\
&& e(t-1), ..., e(t-n_e)] +e(t)
\label{eq:nl_model_narmax_pol_espec}
\end{eqnarray}
onde $e(t)$ indica que todos os efeitos não podem ser bem representados. $F^l\left [ \cdot  \right ]$ é
uma função polinomial de $y(t)$, $u(t)$ e $e(t)$ com grau de não linearidade $l\in \mathbb{N}$. 
Portanto a parte não determinística da equação \eqref{eq:nl_model_narmax_pol_espec} pode ser expandida como
um somatório de termos com grau de não linearidade variando de $1 \le m \le l$. 
Assim sendo, cada termo de grau $m$ poderá conter um valor de grau $p$ do tipo $y(t-i)$
e um fator de grau $(m-p)$ do tipo $u(t-i)$ sendo multiplicado por um parâmetro representado 
por $c_{p,m-p}(n_1, ..., n_m)$. Obtendo: \cite{aguirre}

\begin{equation}
y(t)=\sum_{m=0}^{l}\sum_{p=0}^{m}\sum_{n1, n_m}^{n_y, n_u}c_{p,m-p}(n_1,...,n_m)\prod_{i=1}^{p}y(t-n_i)\prod_{i=p+1}^{m}u(t-n_i)
\end{equation}
sendo que

\begin{equation}
\sum_{n1,n_m}^{n_y,n_u}\equiv \sum_{n_1=1}^{n_y}\cdots\sum_{n_m=1}^{n_y}
\nonumber
\end{equation}
e o limite superior é $n_y$ se o somatório de refere a fatores do tipo $y(t-n_i)$ ou $n_u$ se os fatores forem do tipo
$u(t-n_i)$. 

Os modelos racionais tratados em seguida são diferenciados dos demais apresentados neste trabalho, pois são foto de
estudo posterior nos capítulos seguintes e necessita de um algoritmo especial para a identificação de seus parâmetros,
por isso será tratado em uma seção própria, separado dos outros modelos não lineares, aqui apresentados.
