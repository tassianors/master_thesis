%===============================================================================
\section{Modelos para sistemas não lineares}
\label{sec:nl_models}
%===============================================================================
% ideia aqui é colocar uma pequena introdução sobre modelos.. no mesmo estilo
% que foi para sistemas lineares.

Famílias ou conjuntos de modelos para sistemas podem ser divididos em dois grupos principais, baseados na natureza de
suas não linearidades: não linearidades estáticas, onde a dinâmica do sistema pode ser bem caracterizada por um modelo
linear enquanto que a parte não linear está concentrada ou na entrada ou na saída do sistema de forma estática.
Estes sistemas são chamadosde Wiener para não linearidades na saída do processo ou Hammerstein, quando a
não linearidade está na entrada do processo.

Para os demais casos, onde a não linearidade está na dinâmica do processo existem várias famílias de modelos
que podem ser utilizados como será visto no decorrer deste capítulo. De forma geral todos estes conjuntos
possuem em comum a ideia da escolha de uma base que seja representativa e que possa reduzir a quantidade de
termos a ser identificado e com isso ter uma boa aproximação do sistema real com a menor quantidade de
parâmetros possível na identificação.

Percebe-se então que um mesmo sistema pode ser representado por diversos destes modelos, mas dependendo das
características deste sistema, uma das famílias será mais adequada, por sua base possuir mais
afinidade em representar certos tipos de não linearidades.

Um dos passos mais desafiadores na construção de modelos não lineares é a escolha da estrutura de
modelo. Quando este modelo é não linear, existe uma grande quantidade de opções e com isso o perigo
de escolher um modelo desnecessariamente complexo é evidente. este perigo baseia-se no {\it{principio
da parcimônia}} que basicamente determina que o modelo deve ser o mais simples possível.
\cite{aguirre_maps}

%TODO: achar onde por isso e se nao tem nada melhor no paper para adicionar
%Um problema fundamental referente a modelos parametrizados refere-se ao fato do
%parâmetro poder ou não ser determinado a partir dos valores medidos de entrada e saída. \cite{glad_ljung}

% ===============================================================================
\subsection{Modelos de Wiener e Hammerstein}
\label{sec:nl_models_wiener_hammerstein}
% Aguirre: 334
% ljung 143
%===============================================================================

Modelos de Wiener e Hammerstein são normalmente utilizados em situações onde a dinâmica do sistema pode ser bem
descrita por um sistema linear, mas existem algumas não linearidades estáticas atreladas à entrada
e/ou à saída. Este será o caso de atuadores não lineares com características como saturação, zona morta, entre outros.

Um modelo com não linearidades na entrada é chamado de {\it{modelo de Hammerstein}} e 
para não linearidades na saída chama-se {\it{modelo de Wiener}}. \cite{ljung}

Na Figura (\ref{fig:nl_models_hammerstein_wiener}) observa-se o diagrama de bloco para os modelos
de Hammerstein e Wiener.


\begin{figure}[htbp]
\center
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-1.6)(9.439062,1.6)
\usefont{T1}{ptm}{m}{n}
\rput(0.52453125,1.31){$u(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(3.9557812,1.31){$f(u(t))$}
\usefont{T1}{ptm}{m}{n}
\rput(7.554531,1.35){$y(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(5.8759375,1.15){Modelo}
\usefont{T1}{ptm}{m}{n}
\rput(5.7834377,0.75){Linear}
\usefont{T1}{ptm}{m}{n}
\rput(2.1957812,1.03){$f(\cdot)$}
\psframe[linewidth=0.04,dimen=outer](3.0,1.6)(1.2,0.4)
\psframe[linewidth=0.04,dimen=outer](6.8,1.6)(5.0,0.4)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.0,1.0)(5.0,1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.0,1.0)(1.2,1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.8,1.0)(8.0,1.0)
\usefont{T1}{ptm}{m}{n}
\rput(0.52453125,-0.69){$u(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(3.9357812,-0.69){$z(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(2.0759375,-0.85){Modelo}
\usefont{T1}{ptm}{m}{n}
\rput(1.9834375,-1.25){Linear}
\usefont{T1}{ptm}{m}{n}
\rput(5.9957814,-0.99){$f(\cdot)$}
\psframe[linewidth=0.04,dimen=outer](3.0,-0.4)(1.2,-1.6)
\psframe[linewidth=0.04,dimen=outer](6.8,-0.4)(5.0,-1.6)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.0,-1.0)(5.0,-1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.0,-1.0)(1.2,-1.0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(6.8,-1.0)(8.0,-1.0)
\usefont{T1}{ptm}{m}{n}
\rput(8.204532,-0.65){$y(t)=f(z(t))$}
\end{pspicture} 
}
\caption{Acima: modelo de Hammerstein. Abaixo: Modelo de Wiener.}
\label{fig:nl_models_hammerstein_wiener}
\end{figure}

%===============================================================================
\subsection{Serie de volterra}
\label{sec:nl_models_volterra}
% Aguirre 334
%===============================================================================

Um sistema não linear pode ser descrito pela série de Volterra \eqref{eq:nl_models_volterra}:

\begin{equation}
y(t)=\sum_{j=1}^{\infty}\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}
h_j(\tau_1, ... ,\tau_j) \prod_{i=1}^{j}u(t-\tau_i)d\tau_i
\label{eq:nl_models_volterra}
\end{equation}

onde $h_j$ são generalizações não lineares da resposta ao impulso $h_1(t)$ . Para
um sistema linear com $j=1$ a equação de Volterra se reduz à integral de convolução.
\cite{aguirre}

As expansões funcionais em séries de Volterra relacionam os sinais passados da entrada com o valor atual da saída do
sistema e isso inevitavelmente significa que que um conjunto relativamete elevado de parâmetros é necessário para
descrever até mesmo simples sistemas não lineares e consequentemente apenas poucas aplicações práticas foram apresentadas.
\cite{chen_billings1989}

%===============================================================================
\subsection{Redes Neurais}
\label{sec:nl_models_neurals}
% TODO: Search for bib
%===============================================================================
As redes neurais artificiais são compostas por camadas de neurônios interconectados. A saída de um
neurônio com $n$ entradas é apresentada na equação \eqref{eq:nl_models_neural}

\begin{equation}
x=\emph{f}\left ( \sum_{j=1}^{n}\omega_j x_j +b \right )
\label{eq:nl_models_neural}
\end{equation}

Sendo $b$ (bias) e $\omega_j$ constantes e $\emph{f}(\cdot)$ é chamada função de ativação. A
função de ativação mais comum é: \cite{aguirre}

\begin{equation}
\emph{f}(z)=\frac{1}{1+e^{-z}}
\nonumber
\end{equation}


%===============================================================================
\subsubsection{Redes Neurais multi-camadas}
\label{sec:nl_models_neurals_multilayer}
%===============================================================================

Uma típica rede multi-camadas pode ser descrita como na Figura
(\ref{fig:nl_models_neural_multilayer}).  Na prática redes neurais multi-camadas têm grande apelo no
reconhecimento de padrões. Do ponto de vista teórico os sistemas de redes multi-camadas podem ser
considerados como mapas não lineares onde os elementos das matrizes de peso são os parâmetros.
\cite{narenda_parthasarathy}

\begin{figure}[htbp]
\center
\scalebox{0.85} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.369375)(14.589063,3.329375)
\pscircle[linewidth=0.04,dimen=outer](4.77,2.729375){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(4.804531,2.699375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](7.17,3.329375)(5.97,2.129375)
\usefont{T1}{ptm}{m}{n}
\rput(6.534531,2.759375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](4.77,0.529375){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(4.804531,0.499375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](7.17,1.129375)(5.97,-0.070625)
\usefont{T1}{ptm}{m}{n}
\rput(6.534531,0.559375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](4.77,-1.870625){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(4.804531,-1.900625){$\sum$}
\psframe[linewidth=0.04,dimen=outer](7.17,-1.270625)(5.97,-2.470625)
\usefont{T1}{ptm}{m}{n}
\rput(6.534531,-1.840625){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](10.37,2.729375){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(10.384531,2.679375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](12.77,3.329375)(11.57,2.129375)
\usefont{T1}{ptm}{m}{n}
\rput(12.254531,2.739375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](10.37,0.529375){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(10.384531,0.479375){$\sum$}
\psframe[linewidth=0.04,dimen=outer](12.77,1.129375)(11.57,-0.070625)
\usefont{T1}{ptm}{m}{n}
\rput(12.254531,0.539375){$\gamma$}
\pscircle[linewidth=0.04,dimen=outer](10.37,-1.870625){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(10.384531,-1.920625){$\sum$}
\psframe[linewidth=0.04,dimen=outer](12.77,-1.270625)(11.57,-2.470625)
\usefont{T1}{ptm}{m}{n}
\rput(12.254531,-1.860625){$\gamma$}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(5.17,2.729375)(5.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(5.17,0.529375)(5.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(5.37,-1.870625)(5.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(10.77,2.729375)(11.57,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(10.77,0.529375)(11.57,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(10.77,-1.870625)(11.57,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(12.77,2.729375)(13.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(12.77,0.529375)(13.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(12.77,-1.870625)(13.97,-1.870625)
\usefont{T1}{ptm}{m}{n}
\rput(13.674531,3.039375){$y_1$}
\usefont{T1}{ptm}{m}{n}
\rput(13.674531,0.839375){$y_2$}
\usefont{T1}{ptm}{m}{n}
\rput(13.674531,-1.560625){$y_n$}
\psdots[dotsize=0.12](6.77,-0.470625)
\psdots[dotsize=0.12](6.77,-0.670625)
\psdots[dotsize=0.12](6.77,-0.870625)
\psdots[dotsize=0.12](12.37,-0.470625)
\psdots[dotsize=0.12](12.37,-0.670625)
\psdots[dotsize=0.12](12.37,-0.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,2.729375)(9.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,2.729375)(9.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,2.729375)(9.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,0.529375)(9.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,-1.870625)(9.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,0.529375)(9.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,0.529375)(9.97,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,-1.870625)(9.97,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(7.1873198,-1.870625)(9.97,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7888126,2.7075653)(4.37,2.7075653)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7888126,2.7075653)(4.37,0.51799595)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7888126,2.7075653)(4.37,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,0.529375)(4.37,0.529375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,-1.850625)(4.37,-1.850625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,0.529375)(4.37,2.729375)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,0.529375)(4.37,-1.870625)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,-1.850625)(4.37,0.5389402)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(1.7871279,-1.850625)(4.37,2.729375)
\usefont{T1}{ptm}{m}{n}
\rput(1.4745313,3.039375){$u_1$}
\usefont{T1}{ptm}{m}{n}
\rput(1.4745313,0.839375){$u_2$}
\usefont{T1}{ptm}{m}{n}
\rput(1.4745313,-1.560625){$u_n$}
\usefont{T1}{ptm}{m}{n}
\rput(1.4845313,-3.160625){$\text{Entrada}$}
\usefont{T1}{ptm}{m}{n}
\rput(6.7345314,-3.160625){$\text{Camada Oculta}$}
\usefont{T1}{ptm}{m}{n}
\rput(12.444531,-3.160625){$\text{Camada Saída}$}
\end{pspicture} 
}

\caption{Rede neural multi-camadas.}
\label{fig:nl_models_neural_multilayer}
\end{figure}

A saída do sistema para uma rede multi-camadas pode ser descrita como:

\begin{equation}
y(t)=f_s\left \{ \sum_{i=1}^{m} \omega_i f_i \left ( \sum_{j=1}^{n}\omega_{ij} x_j + b_i \right ) +
b_s \right \}
\label{eq:nl_models_neural_multilayer}
\end{equation}

Sendo que $f_s$ é a função de ativação do neurônio da camada de saída. Esta função não precisa ser
igual a $f_i$, $i=1, \ldots , m$ que por sua vez não precisam ser iguais entre si. As constantes $b_s$ são os termos
de polarização dos neurônios da camada de saída, $\omega_i$ são os pesos da saída de cada neurônio da
camada oculta e $\omega_{ij}$ são os pesos da entrada $j$, vista pelo $i-$ésimo neurônio da camada
oculta. \cite{aguirre}

%===============================================================================
\subsubsection{Redes Neurais recorrentes}
\label{sec:nl_models_neurals_multilayer}
%===============================================================================

Redes neurais recorrentes, trabalho introduzido por Hopfield em \cite{hopfield} provê uma
alternativa para o reconhecimento de padrões. O método proposto consiste em ter uma rede neural de
apenas uma camada adicionada de uma realimentação com um atraso de tempo como apresentado na Figura
(\ref{fig:nl_models_neural_recurrent}). \cite{narenda_parthasarathy}

\begin{figure}[htbp]
\center
\scalebox{0.85} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-5.4)(11.04,5.4)
\usefont{T1}{ptm}{m}{n}
\rput(8.015624,4.91){$\gamma$}
\usefont{T1}{ptm}{m}{n}
\rput(8.015624,3.31){$\gamma$}
\usefont{T1}{ptm}{m}{n}
\rput(8.215624,0.71){$\gamma$}
\usefont{T1}{ptm}{m}{n}
\rput(5.073437,-0.89){$z^{-1}$}
\pscircle[linewidth=0.04,dimen=outer](6.42,4.8){0.4}
\pscircle[linewidth=0.04,dimen=outer](6.42,0.6){0.4}
\pscircle[linewidth=0.04,dimen=outer](6.42,3.2){0.4}
\usefont{T1}{ptm}{m}{n}
\rput(6.395625,0.61){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(6.395625,3.21){$\sum$}
\usefont{T1}{ptm}{m}{n}
\rput(6.395625,4.81){$\sum$}
\psdots[dotsize=0.12](5.82,4.8)
\psdots[dotsize=0.12](5.82,3.2)
\psdots[dotsize=0.12](5.82,0.6)
\psdots[dotsize=0.12](2.82,4.8)
\psdots[dotsize=0.12](2.82,3.2)
\psdots[dotsize=0.12](2.82,0.6)
\psline[linewidth=0.04cm](2.82,4.8)(5.82,4.8)
\psline[linewidth=0.04cm](2.82,4.8)(5.82,3.1916058)
\psline[linewidth=0.04cm](2.82,4.8)(5.82,0.61817497)
\psline[linewidth=0.04cm](2.82,0.61817497)(5.82,4.8)
\psline[linewidth=0.04cm](2.82,3.1916058)(5.82,4.8)
\psline[linewidth=0.04cm](2.82,3.1916058)(5.82,3.1916058)
\psline[linewidth=0.04cm](2.82,3.1916058)(5.82,0.61817497)
\psline[linewidth=0.04cm](2.82,0.61817497)(5.82,3.1916058)
\psline[linewidth=0.04cm](2.82,0.61817497)(5.82,0.61817497)
\psline[linewidth=0.04cm](8.8,0.6)(9.42,0.6)
\psline[linewidth=0.04cm](9.4,-1.0)(5.8,-1.0)
\psline[linewidth=0.04cm](4.62,-1.0)(1.6,-1.0)
\psline[linewidth=0.04cm](1.62,0.6)(2.82,0.6)
\psline[linewidth=0.04cm](6.82,0.6)(7.62,0.6)
\psline[linewidth=0.04cm](5.82,0.6)(6.0,0.6)
\psline[linewidth=0.04cm](6.82,3.2)(7.62,3.2)
\psline[linewidth=0.04cm](5.82,3.2)(6.02,3.2)
\psline[linewidth=0.04cm](5.82,4.8)(6.02,4.8)
\psline[linewidth=0.04cm](6.82,4.8)(7.62,4.8)
\psline[linewidth=0.04cm](8.8,3.2)(10.22,3.2)
\psline[linewidth=0.04cm](5.8,-2.6)(10.2,-2.6)
\psline[linewidth=0.04cm](8.8,4.8)(11.02,4.8)
\psline[linewidth=0.04cm](11.0,-4.8)(5.8,-4.8)
\psline[linewidth=0.04cm](4.62,-2.6)(0.8,-2.6)
\psline[linewidth=0.04cm](0.8,3.2)(2.82,3.2)
\psline[linewidth=0.04cm](2.82,4.8)(0.0,4.8)
\psline[linewidth=0.04cm](0.0,-4.8)(4.62,-4.8)
\psdots[dotsize=0.12](8.22,2.2)
\psdots[dotsize=0.12](8.22,2.0)
\psdots[dotsize=0.12](8.22,1.8)
\psdots[dotsize=0.12](5.22,-3.4)
\psdots[dotsize=0.12](5.22,-3.6)
\psdots[dotsize=0.12](5.22,-3.8)
\usefont{T1}{ptm}{m}{n}
\rput(9.864062,5.11){$x_1(t+1)$}
\usefont{T1}{ptm}{m}{n}
\rput(9.864062,3.51){$x_2(t+1)$}
\usefont{T1}{ptm}{m}{n}
\rput(9.864062,0.91){$x_n(t+1)$}
\usefont{T1}{ptm}{m}{n}
\rput(2.0040624,5.11){$x_1(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(2.0040624,3.51){$x_2(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(2.0040624,0.91){$x_n(t)$}
\usefont{T1}{ptm}{m}{n}
\rput(4.3840623,5.11){$\omega_i$}
\usefont{T1}{ptm}{m}{n}
\rput(5.073437,-2.49){$z^{-1}$}
\usefont{T1}{ptm}{m}{n}
\rput(5.073437,-4.69){$z^{-1}$}
\psframe[linewidth=0.04,dimen=outer](8.8,5.4)(7.6,4.2)
\psframe[linewidth=0.04,dimen=outer](8.8,3.8)(7.6,2.6)
\psframe[linewidth=0.04,dimen=outer](8.8,1.2)(7.6,0.0)
\psframe[linewidth=0.04,dimen=outer](5.8,-0.4)(4.6,-1.6)
\psframe[linewidth=0.04,dimen=outer](5.8,-2.0)(4.6,-3.2)
\psframe[linewidth=0.04,dimen=outer](5.8,-4.2)(4.6,-5.4)
\psline[linewidth=0.04cm](11.0,4.8)(11.0,-4.8)
\psline[linewidth=0.04cm](10.2,3.2)(10.2,-2.6)
\psline[linewidth=0.04cm](9.4,0.6)(9.4,-1.0)
\psline[linewidth=0.04cm](1.6,0.6)(1.6,-1.0)
\psline[linewidth=0.04cm](0.8,-2.6)(0.8,3.2)
\psline[linewidth=0.04cm](0.0,4.8)(0.0,-4.8)
\end{pspicture} 
}
\caption{Rede neural recorrentes.}
\label{fig:nl_models_neural_recurrent}
\end{figure}

%===============================================================================
\subsection{Funções Radiais de Base}
\label{sec:nl_models_radiais}
% Aguirre 337
%===============================================================================

Funções radiais de base  ({\it{RBF - Radial basis functions}})  são uma tradicional técnica para
interpolação em espaços multidimensional \cite{chen_billings_narmax} e podem ser descritas como
mapeamentos do tipo:

\begin{equation}
f(y)=w_0+\sum_{i}w_i \phi (\left \| y-c_i \right \|)
\label{eq:nl_models_rbf}
\end{equation}

Sendo que $y \in \mathbb{R}^{d_e}$ ($d_e$ é conhecido como dimensão de imersão),
$\left \| \cdot \right \|$ é a norma euclidiana, $w_i \in \mathbb{R}$ são pesos, 
$c_i \in \mathbb{R}^{d_e}$ são centros e $\phi(\cdot):\mathbb{R}^+ \to \mathbb{R}$ 
é uma função, normalmente escolhida a priori, como por exemplo: \cite{aguirre}

\begin{equation}
\phi(\left \| y-c_i \right \|)= exp\left ( -\frac{\left \| y-c_i \right \|^2}{\sigma_i^2} \right )
\nonumber
\end{equation}

Outras funções de base usadas são apresentadas na Tabela (\ref{table:nl_models_rbf})

\begin{table*}[htbp]
\begin{center}
\caption{Algumas funções Radiais de base comumente utilizadas.}
\label{table:nl_models_rbf}
\begin{tabular}{ll}
\hline
        Nome & Função   \\
\hline
        Multi quadrática inversa  & $\phi(r)=(r^2+\sigma ^2)^{-1/2}$ \\ 
        Linear                    & $\phi(r)=r$                      \\ 
        Cúbica                    & $\phi(r)=r^3$                    \\ 
        Multi-quadrática           & $\phi(r)=\sqrt{r^2+\sigma^2}$    \\ 
        {\it{Thin - plate spline}} & $\phi(r)=r^2\; \text{log}(r)$   \\ 
\hline
\end{tabular}
\end{center}
\end{table*}

onde $r=\left \| y-c_i \right \|$ e $\sigma$ definem a largura do chapéu no caso de
funções Gausianas e das multi-quadráticas, como pode ser visto na figura (\ref{fig:nl_models_rbf}).

\begin{figure}[htbp]
	\center
	\includegraphics[width=0.8\columnwidth]{figures/nl_models_rbf.eps}
	\caption{Função multiquadráticas inversa para alguns valores de $\sigma$.}
	\label{fig:nl_models_rbf}
\end{figure}

Este tipo de representação tem boas propriedades locais e pode ser interpretada como 
uma técnica de interpolação global. Funções radiais de base são casos particulares
de redes neurais, porem neste caso lineares nos parâmetros $w_i$.\cite{aguirre} 

No contexto de identificação de sistemas é comum adicionar termos auto-regressivos
lineares, bem como termos de entrada à equação \eqref{eq:nl_models_rbf} resultando em:

\begin{equation}
y(k)=w_0 + \sum_{i}w_i \phi(\left \| \mathbf{y}(k-1)-c_i \right \|)+\sum_{i=1}^{n_y}a_i y(k-i)+\sum_{i=1}^{n_u}b_i u(k-i)+e(k)
\nonumber
\end{equation}

Sendo $\mathbf{y}(k-1)=\begin{bmatrix}
y(k-1) & ... & y(k-n_y) & u(k-1) & ... & u(k-n_u)
\end{bmatrix}$.
