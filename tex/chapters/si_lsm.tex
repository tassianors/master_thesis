%===============================================================================
\section{Estimativa de parâmetros}
\label{sec:sys_ident_parameters_estimation}
%===============================================================================


%===============================================================================
\subsection{Preditores}
\label{sec:si_par_estim_preditors}
%===============================================================================

Considere o sistema apresentado em (\ref{eq:si_modeling_lti}). Assume-se que os
sinais $y(p)$ e $u(p)$ são conhecidos para $p \le t-1$. A partir de (\ref{eq:si_par_estim_vs})
tem-se que até $\upsilon (p)$ é definido. O objetivo então é prever $y(t)$ como
em (\ref{eq:si_par_estim_yt}).

\begin{equation}
\upsilon (p)=y(p) -G(q)u(p)
\label{eq:si_par_estim_vs}
\end{equation}

\begin{equation}
y(t)=G(q)u(t)+\upsilon (t)
\label{eq:si_par_estim_yt}
\end{equation}

Fazendo-se as substituições necessárias chega-se ao estimador (\ref{eq:si_par_estim_predictor})
onde enfatiza-se a dependência com o parâmetro $\theta$. \cite{ljung}

\begin{equation}
\hat{y}(t|\theta)=H^{-1}(q,\theta)G(q,\theta)u(t)+\left [ 1- H^{-1}(q,\theta)\right ]y(t)
\label{eq:si_par_estim_predictor}
\end{equation}

O erro de predição é intuitivamente descrito como em (\ref{eq:si_par_estim_err_predic}).
Este erro é amplamente utilizado para determinar a qualidade da estimativa que se 
encontra. Como será visto a seguir.

\begin{equation}
\varepsilon (t| \theta)=y(t)-\hat{y}(t|\theta)
\label{eq:si_par_estim_err_predic}
\end{equation}

Que pode ser escrito como 

\begin{equation}
\varepsilon (t,\theta)=\frac{D(q)}{C(q)}\left [ A(q)y(t) - \frac{B(q)}{F(q)}u(t)\right ]
\nonumber
\end{equation}

Definimos então duas variáveis auxiliares \eqref{eq:si_estim_predict_w} e \eqref{eq:si_estim_predict_v}

\begin{equation}
w(t,\theta)=\frac{B(q)}{F(q)}u(t)
\label{eq:si_estim_predict_w}
\end{equation}

\begin{equation}
\upsilon (t,\theta)=A(q)y(t)-w(t,\theta)
\label{eq:si_estim_predict_v}
\end{equation}

Então:

\begin{equation}
\varepsilon (t,\theta)=y(t)-\hat{y}(t|\theta)=\frac{D(q)}{C(q)}\upsilon (t,\theta)
\nonumber
\end{equation}

Desta forma introduz-se o vetor de estados \eqref{eq:si_estim_predic_state}.\cite{ljung}


\begin{eqnarray*}
  \varphi (t,\theta)= & [ -y(t-1), ... , -y(t-n_a), u(t-1), ..., u(t-n_b), \\
   &  -w(t-1, \theta),..., -w(t-n_f, \theta), \varepsilon (t-1, \theta), ...,\varepsilon (t-n_c, \theta), \\
   &  \upsilon (t-1, \theta), ...,\upsilon (t-n_d, \theta) ]^T
\label{eq:si_estim_predic_state}
\end{eqnarray*}

E o vetor de parametros:

\begin{equation}
\theta = \left [ a_1 \; ... \; a_{na} \; b_1 \; ... \; b_{nb} \; f_1 \; ... \; f_{nf} \; c_1 \; ... \; c_{nc} \; d_1 \; ... \; d_{nd}\right ]^T
\label{eq:si_estim_predict_theta}
\end{equation}

Chega-se então a seguinte equação para o erro de predição:

\begin{equation}
\varepsilon (t,\theta)=y(t)-\theta^T\varphi (t,\theta)
\label{eq:si_estim_predict_erro}
\end{equation}

Onde:

\begin{equation}
\hat{y}(t| \theta)=\theta^T\varphi (t,\theta)=\varphi ^T(t,\theta)\theta
\nonumber
\end{equation}

%===============================================================================
\subsection{Método dos mínimos quadrados}
\label{sec:si_par_estim_lsm}
%===============================================================================

Existem diversos métodos para a estimativa de parâmetros. O mais conhecido, remete
ao ano de 1809 utilizado por Gauss para determinação da orbita dos planetas. 
\cite{system_identification}

A regressão linear é o tipo mais simples de modelo paramétrico. A estrutura do modelo
pode ser descrita como em (\ref{eq:si_lsm_single_var}).

\begin{equation}
y(t)=\varphi ^T(t)\theta
\label{eq:si_lsm_single_var}
\end{equation}

Onde $y(t)$ é chamada de {\it{variável regredida}} e é a variável medida do processo.
$\varphi (t)$ é comumente chamado de {\it{variável de regressão}} e $\theta$ é o vetor de
parâmetros.

O modelo apresentado em (\ref{eq:si_lsm_single_var}) é facilmente estendido para o modelo
multivariáveis (\ref{eq:si_lsm_multi_var}).

\begin{equation}
y(t)=\Phi ^T(t)\theta
\label{eq:si_lsm_multi_var}
\end{equation}

Onde $y(t)$ é um vetor de $p$ posições, $\Phi(t)$ uma matriz $n \times p$ e $\theta$ é um 
vetor de $N$ posições.

A ideia é encontrar uma estimativa $\hat{\theta}$ dos parâmetros de $\theta$ a partir de medidas
de $y(1),\varphi(1),\cdots,y(N),\varphi(N)$. 

A partir de (\ref{eq:si_par_estim_err_predic}) e (\ref{eq:si_lsm_multi_var}) temos 

\begin{equation}
\varepsilon (t)=y(t)-\varphi ^T(t)\theta
\nonumber
\end{equation}

A {\it{estimativa dos mínimos quadrados}} de $\theta$ é definido como o vetor $\hat{\theta}$ 
que minimiza a função custo (\ref{eq:si_par_etim_lsm_v}).

\begin{equation}
V(\theta)=\frac{1}{2}\sum_{t=1}^{N}\varepsilon ^2(t)=\frac{1}{2}\varepsilon^T\varepsilon=\frac{1}{2}\left \| \varepsilon \right \|
\label{eq:si_par_etim_lsm_v}
\end{equation}

O valor de $\hat{\theta}$ que minimiza (\ref{eq:si_lsm_multi_var}) é dado por:

\begin{equation}
\hat{\theta}=(\varphi ^T \varphi )^{-1}\varphi  ^T y
\label{eq:si_par_etim_lsm_theta}
\end{equation}

O mínimo da função custo fica como em:

\begin{equation}
\underset{\theta}{min}\;V(\theta)=V(\hat{\theta})=\frac{1}{2}\left [ y^Ty-y^T\varphi (\varphi ^T \varphi )^{-1}\varphi ^T y \right ]
\end{equation}

%===============================================================================
\subsection{Identificabilidade de sistemas}
\label{sec:si_par_estim_sys_ident}
%===============================================================================


A identificabilidade de um sistema pode ser definido pelo teorema (\ref{theorem:identificability}).

\begin{theorem} 
\cite{ljung}

Considerando a estrutura de modelo $\mathcal{M}$ correspondente a: 
\begin{equation}
A(q)y(t)=\frac{B(q)}{F(q)}u(t)+\frac{C(q)}{D(q)}e(t)
\label{eq:si_par_estim_sys_theorem}
\end{equation}

e com $\theta$ como em \eqref{eq:si_estim_predict_theta} sendo o coeficiente dos polinômios envolvidos.
Os graus dos polinômios são $n_a$, $n_b$ e assim por diante. A estrutura deste modelo 
é globalmente identificável se, e somente se, todos os itens de (i) até (vi) forem verdadeiros.

\begin{enumerate}[(i)]
\item Não existe fator comum para todos os $z^{na}A^*(z), z^{nb}B^*(z), and, \; z^{nc}C^*(z)$.
\item Não existe fator comum para $z^{nb}B^*(z), and, \; z^{nf}F^*(z)$.
\item Não existe fator comum para $z^{nc}C^*(z), and, \; z^{nd}D^*(z)$.
\item Se $n_a \geq 1$ então não pode existir fator comum entre $z^{nf}F^*(z), and, \; z^{nd}D^*(z)$.
\item Se $n_d \geq 1$ então não pode existir fator comum entre $z^{na}A^*(z), and, \; z^{nb}B^*(z)$.
\item Se $n_f \geq 1$ então não pode existir fator comum entre $z^{na}A^*(z), and, \; z^{nc}C^*(z)$.
\end{enumerate}

Os polinômios com $*$ correspondem a $\theta^*$
\label{theorem:identificability}
\end{theorem}

Note que as condições de (i) até (vi) são satisfeitas para o caso comum de \eqref{eq:si_par_estim_sys_theorem}.
Observe também que nenhuma das condições pode ser violada por qualquer \"especial\" valor de $\theta^*$, posto na
hiper-superfície de $\Re ^d$. Isso nos leva ao seguinte corolário:

\begin{cor} 
A estrutura de modelo apresentada em \eqref{eq:si_par_estim_sys_theorem} é globalmente identificável.
\label{corollary:identificability}
\end{cor} 

A partir do Corolário (\ref{corollary:identificability}) temos que a identificação de sistemas
utilizando os modelos descritos pela Tabela \ref{table:si_modeling_models} são globalmente identificáveis
se tivermos os dados de entrada suficientemente informativos.

%===============================================================================
\subsection{Incertezas nos parâmetros estimados}
\label{sec:si_par_estim_uncertanties}
%===============================================================================

O desejo principal para a estimativa de um sistema é que este não possua erros, ou ao menos que este erro, 
se existir, seja o menor possível. Para tanto é necessário caracterizar este tipo de informação do sistema.
Existem dois tipos principais de erro na estimativa de parâmetros. Um deles é o {\it{erro de variância}} e
outro é o {\it{erro de polarização}}.

Seja, $\theta^*$ o limite de convergência para a minimização do erro de predição:

\begin{equation}
\lim_{N \rightarrow \infty }\hat{\theta}_N = \theta^*
\label{eq:si_par_estim_under_theta}
\end{equation}

Sejam os vetores:

\begin{equation}
	Q(q)=\begin{bmatrix}
G_0(q) & H_0(q)
\end{bmatrix}^T
\nonumber
\end{equation}


\begin{equation}
\hat{Q}_N(q)=\begin{bmatrix}
G_0(q, \hat{\theta}_N) & H_0(q, \hat{\theta}_N)
\end{bmatrix}^T
\nonumber
\end{equation}

A qualidade da estimativa pode ser calculada em termos da diferença entre o processo real $Q(q)_0$ e o 
melhor sistema que o modelo pode atingir (quando a quantidade de dados é ilimitada ($N\rightarrow \infty$)) tem-se
então a estimativa do modelo $G(q)_N$.\cite{campestrini, solari}

\begin{equation}
\Delta Q_N(q) \equiv \hat{Q}_N-Q(q)
\label{eq:si_par_estim_under_diff}
\end{equation}

Define-se então:

\begin{equation}
Q^*(q) = \left [ G(q, \theta^*) \; H(q, \theta^*) \right ]^T
\label{eq:si_par_estim_under_q*}
\end{equation}

Entende-se por $Q(q)^*$ como a melhor aproximação que o método pode proporcionar (com $N \rightarrow \infty$) ou 
como sendo a média de todas as estimativas efetuadas.

Adicionando e subtraindo a \eqref{eq:si_par_estim_under_q*} na equação \eqref{eq:si_par_estim_under_diff}
chega-se a definição do erro de polarização e de variância \eqref{eq:si_par_estim_under_errors}.

\begin{equation}
\Delta Q_N(q) \equiv \underset{\text{Erro de variância}}{\underbrace{\hat{Q}_N-Q^*(q)}}+  \underset{\text{Erro de variância}}{\underbrace{Q^*(q)-Q(q)}}
\label{eq:si_par_estim_under_errors}
\end{equation}

A partir da \eqref{eq:si_par_estim_under_errors} compreende-se o erro de polarização como sendo a distância
entre a melhor aproximação possível e o valor real para o parâmetro. Já o erro de variância é a média que cada uma
das estimativas está distante do valor ótimo possível para a estimativa.

\begin{theorem}
Supondo um sistema linear $\mathcal{M}$ parametrizado com em \eqref{eq:si_par_estim_theorem_sys}
e  que os dados de entrada sejam suficientemente informativos.

\begin{equation}
\theta=\begin{bmatrix}
\rho \\ 
\eta 
\end{bmatrix}\;\;
G(q,\theta)=G(q,\rho), \;\;
H(q,\theta)=G(q,\eta)
\label{eq:si_par_estim_theorem_sys}
\end{equation}

Considerando que o sistema opera em malha aberta, ou seja:

\begin{equation}
u(t) \;e\; e_0(t)\text{ são independentes.}
\nonumber
\end{equation}

Seja:

\begin{equation}
\hat{\theta}_N=\begin{bmatrix}
\hat{\rho}_N \\ 
\hat{\eta}_N
\end{bmatrix}
\nonumber
\end{equation}

Obtido pelo método de predição apresentado %%achar a ref 
obtém-se:

\begin{equation}
\begin{matrix}
G(e^{j\omega},\hat{\theta}_N)\rightarrow G_0(e^{j\omega})\\ 
\text{quando:}\\
N\rightarrow \infty
\end{matrix}
\label{eq:si_par_estim_theorem_end}
\end{equation}


\end{theorem}

%===============================================================================
\subsection{Considerações Finais}
\label{sec:si_par_estim_conclusions}
%===============================================================================

