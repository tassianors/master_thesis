%===============================================================================
\section{Estimativa de parametros}
\label{sec:sys_ident_parameters_estimation}
%===============================================================================


%===============================================================================
\subsection{Preditores}
\label{sec:si_par_estim_preditors}
%===============================================================================

Considere o sistema apresentado em (\ref{eq:si_modeling_lti}). Assume-se que os
sinais $y(p)$ e $u(p)$ são conhecidos para $p \le t-1$. A partir de (\ref{eq:si_par_estim_vs})
tem-se que até $\upsilon (p)$ é definido. O objetivo então é prever $y(t)$ como
em (\ref{eq:si_par_estim_yt}).

\begin{equation}
\upsilon (p)=y(p) -G(q)u(p)
\label{eq:si_par_estim_vs}
\end{equation}

\begin{equation}
y(t)=G(q)u(t)+\upsilon (t)
\label{eq:si_par_estim_yt}
\end{equation}

Fazendo-se as substiruições necessárias chega-se ao estimador (\ref{eq:si_par_estim_predictor})
onde enfatiza-se a dependência com o parametro $\theta$. \cite{ljung}

\begin{equation}
\hat{y}(t|\theta)=H^{-1}(q,\theta)G(q,\theta)u(t)+\left [ 1- H^{-1}(q,\theta)\right ]y(t)
\label{eq:si_par_estim_predictor}
\end{equation}

O erro de predição é intuitivamente descrito como em (\ref{eq:si_par_estim_err_predic}).
Este erro é amplamente utilizado para determinar a qualidade da estimativa que se 
encontra. Como será visto a seguir.

\begin{equation}
\varepsilon (t| \theta)=y(t)-\hat{y}(t|\theta)
\label{eq:si_par_estim_err_predic}
\end{equation}


%===============================================================================
\subsection{Método dos mínimos quadrados}
\label{sec:si_par_estim_lsm}
%===============================================================================

Existem diversos métodos para a estimativa de parâmetros. O mais conhecido, remete
ao ano de 1809 utilizado por Gauss para determinação da orbita dos planetas. 
\cite{system_identification}

A regressão linear é o tipo mais simples de modelo paramétrico. A estrutura do modelo
pode ser descrita como em (\ref{eq:si_lsm_single_var}).

\begin{equation}
y(t)=\varphi ^T(t)\theta
\label{eq:si_lsm_single_var}
\end{equation}

Onde $y(t)$ é chamada de {\it{variável regredida}} e é a variável medida do processo.
$\varphi (t)$ é comumente chamado de {\it{variável de regressão}} e $\theta$ é o vetor de
parâmetros.

O modelo apresentado em (\ref{eq:si_lsm_single_var}) é facilmente estendido para o modelo
multivariáveis (\ref{eq:si_lsm_multi_var}).

\begin{equation}
y(t)=\Phi ^T(t)\theta
\label{eq:si_lsm_multi_var}
\end{equation}

Onde $y(t)$ é um vetor de $p$ posições, $\Phi(t)$ uma matriz $n \times p$ e $\theta$ é um 
vetor de $n$ posições.

A ideia é encontrar uma estimativa $\hat{\theta}$ dos parâmetros de $\theta$ a partir de medidas
de $y(1),\varphi(1),\cdots,y(N),\varphi(N)$. 

A partir de (\ref{eq:si_par_estim_err_predic}) e (\ref{eq:si_lsm_multi_var}) temos 

\begin{equation}
\varepsilon (t)=y(t)-\Phi ^T(t)\theta
\nonumber
\end{equation}

A {\it{ estimativa dos mínimos quadrados}} de $\theta$ é defnido como o vetor $\hat{\theta}$ 
que minimiza a função custo (\ref{eq:si_par_etim_lsm_v}).

\begin{equation}
V(\theta)=\frac{1}{2}\sum_{t=1}^{N}\varepsilon ^2(t)=\frac{1}{2}\varepsilon^T\varepsilon=\frac{1}{2}\left \| \varepsilon \right \|
\label{eq:si_lsm_multi_var}
\end{equation}


